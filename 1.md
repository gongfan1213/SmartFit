# requirements.txt
```python
torch
torchvision
transformers
datasets
albumentations
opencv-python
Pillow
matplotlib
huggingface-hub
```
# config.yaml

```js
batch_size: 8                        # Batch size
learning_rate: 0.00005               # Learning rate
epochs: 100                          # Number of epochs
save_path: 'path_to_save_model'      # Checkpoint save path
load_checkpoint: null                # Checkpoint to load (or null)
log_dir: 'path_to_tensorboard_logs'  # TensorBoard log dir
```
# train.py

```js
import torch
from torch.utils.data import DataLoader
from transformers import (
    Mask2FormerConfig,
    Mask2FormerForUniversalSegmentation,
    Mask2FormerImageProcessor,
)
from torch.utils.tensorboard import SummaryWriter
from tqdm.auto import tqdm
import evaluate
import albumentations as A
from Data import ImageSegmentationDataset


class SegmentationTrainer:
    def __init__(
        self,
        train_dataset,
        val_dataset,
        id2label,
        batch_size=4,
        lr=5e-5,
        epochs=10,
        save_path="model_checkpoint",
        load_checkpoint=None,
        log_dir="logs",
    ):
        """
        Args:
            train_dataset (Dataset): Training dataset.
            val_dataset (Dataset): Validation dataset.
            id2label (dict): Mapping of label IDs to label names.
            batch_size (int, optional): Batch size for DataLoader. Defaults to 4.
            lr (float, optional): Learning rate. Defaults to 5e-5.
            epochs (int, optional): Number of training epochs. Defaults to 10.
            save_path (str, optional): Directory to save model checkpoints. Defaults to "model_checkpoint".
            load_checkpoint (str, optional): Path to load a pre-trained model checkpoint. Defaults to None.
            log_dir (str, optional): Directory to save TensorBoard logs. Defaults to "logs".
        """
        self.id2label = id2label
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

        # Initialize image processor
        self.processor = Mask2FormerImageProcessor(
            ignore_index=0,
            reduce_labels=False,
            do_resize=False,
            do_rescale=False,
            do_normalize=False,
        )

        # Define data augmentation and normalization transforms
        train_transform = A.Compose(
            [
                A.LongestMaxSize(max_size=1333),
                A.Resize(width=512, height=512),
                A.HorizontalFlip(p=0.5),
                A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
            ]
        )

        test_transform = A.Compose(
            [
                A.Resize(width=512, height=512),
                A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
            ]
        )

        # Initialize datasets and DataLoader
        self.train_dataset = ImageSegmentationDataset(
            dataset=train_dataset, transform=train_transform
        )
        self.val_dataset = ImageSegmentationDataset(
            dataset=val_dataset, transform=test_transform
        )
        self.train_loader = DataLoader(
            self.train_dataset,
            batch_size=batch_size,
            shuffle=True,
            collate_fn=self.collate_fn,
        )
        self.val_loader = DataLoader(
            self.val_dataset,
            batch_size=batch_size,
            shuffle=False,
            collate_fn=self.collate_fn,
        )

        # Initialize model
        self.model = self.get_model(load_checkpoint)
        self.model.to(self.device)

        # Initialize optimizer and learning rate scheduler
        self.optimizer = torch.optim.AdamW(
            self.model.parameters(), lr=lr, weight_decay=0.01
        )
        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
            self.optimizer,
            mode="min",
            factor=0.1,
            patience=3,
            threshold=0.0001,
            threshold_mode="rel",
            cooldown=0,
            min_lr=0,
            verbose=True,
        )

        # Initialize metric for evaluation
        self.metric = evaluate.load("mean_iou")
        self.epochs = epochs
        self.save_path = save_path

        # Initialize TensorBoard SummaryWriter
        self.writer = SummaryWriter(log_dir=log_dir)

    def get_model(self, load_checkpoint):
        """Load the model, either from a checkpoint or by initializing a new model."""
        if load_checkpoint is not None:
            print("Loading Checkpoint!")
            model = Mask2FormerForUniversalSegmentation.from_pretrained(load_checkpoint)
        else:
            # Load and configure the model
            config = Mask2FormerConfig.from_pretrained(
                "facebook/mask2former-swin-small-ade-semantic"
            )
            config.id2label = self.id2label
            config.label2id = {label: idx for idx, label in self.id2label.items()}

            model = Mask2FormerForUniversalSegmentation.from_pretrained(
                "facebook/mask2former-swin-small-ade-semantic",
                config=config,
                ignore_mismatched_sizes=True,
            )

        return model

    def collate_fn(self, batch):
        """Collate function to process batches of data."""
        inputs = list(zip(*batch))
        images = inputs[0]
        segmentation_maps = inputs[1]
        batch = self.processor(
            images,
            segmentation_maps=segmentation_maps,
            return_tensors="pt",
        )
        batch["original_images"] = inputs[2]
        batch["original_segmentation_maps"] = inputs[3]

        return batch

    def train(self):
        """Train the model."""
        for epoch in range(self.epochs):
            self.model.train()
            epoch_loss = 0.0

            for idx, batch in enumerate(tqdm(self.train_loader)):
                self.optimizer.zero_grad()

                # Forward pass
                outputs = self.model(
                    pixel_values=batch["pixel_values"].to(self.device),
                    mask_labels=[
                        labels.to(self.device) for labels in batch["mask_labels"]
                    ],
                    class_labels=[
                        labels.to(self.device) for labels in batch["class_labels"]
                    ],
                )

                # Backward pass
                loss = outputs.loss
                loss.backward()
                self.optimizer.step()

                epoch_loss += loss.item()

            # Log average training loss
            avg_train_loss = epoch_loss / len(self.train_loader)
            self.writer.add_scalar("Loss/Train", avg_train_loss, epoch)
            print(f"Epoch {epoch+1} complete. Avg Training Loss: {avg_train_loss:.4f}")

            # Validation
            avg_val_loss = self.validate(epoch)
            self.writer.add_scalar("Loss/Validation", avg_val_loss, epoch)
            print(f"Validation Loss: {avg_val_loss:.4f}")

            # Update learning rate based on validation loss
            self.scheduler.step(avg_val_loss)

            # Save the model if it improves
            self.save_model(epoch, avg_train_loss, avg_val_loss)

        self.writer.close()

    def validate(self, epoch):
        """Validate the model on the validation dataset."""
        self.model.eval()
        val_loss = 0.0
        with torch.no_grad():
            for idx, batch in enumerate(tqdm(self.val_loader)):
                outputs = self.model(
                    pixel_values=batch["pixel_values"].to(self.device),
                    mask_labels=[
                        labels.to(self.device) for labels in batch["mask_labels"]
                    ],
                    class_labels=[
                        labels.to(self.device) for labels in batch["class_labels"]
                    ],
                )
                val_loss += outputs.loss.item()

                # Post-process the output for evaluation
                original_images = batch["original_images"]
                target_sizes = [
                    (image.shape[0], image.shape[1]) for image in original_images
                ]
                predicted_segmentation_maps = (
                    self.processor.post_process_semantic_segmentation(
                        outputs, target_sizes=target_sizes
                    )
                )

                # Add batch results to metric
                ground_truth_segmentation_maps = batch["original_segmentation_maps"]
                self.metric.add_batch(
                    references=ground_truth_segmentation_maps,
                    predictions=predicted_segmentation_maps,
                )

            # Compute and print mean IoU
            mean_iou = self.metric.compute(
                num_labels=len(self.id2label), ignore_index=0
            )["mean_iou"]
            print("Mean IoU:", mean_iou)

        avg_val_loss = val_loss / len(self.val_loader)
        return avg_val_loss

    def save_model(self, epoch, train_loss, val_loss):
        """Save the model if validation loss improves."""
        save_criteria = epoch == 0 or val_loss < getattr(
            self, "best_val_loss", float("inf")
        )
        if save_criteria:
            print(f"Saving model at epoch {epoch+1}")
            self.model.save_pretrained(f"{self.save_path}/epoch_{epoch+1}")
            self.best_val_loss = val_loss
```


# running_training.py

```js
import yaml
from scripts.train import SegmentationTrainer
from datasets import load_dataset
from huggingface_hub import login, hf_hub_download
import json


def load_config(config_file):
    """
    Load configuration settings from a YAML file.

    Args:
        config_file (str): Path to the YAML configuration file.

    Returns:
        dict: Configuration parameters.
    """
    with open(config_file, "r") as file:
        config = yaml.safe_load(file)
    return config


if __name__ == "__main__":
    # Load configuration from YAML file
    config = load_config("config.yaml")

    # Specify the repository ID and the filename for id2label mapping
    repo_id = "EduardoPacheco/FoodSeg103"
    filename = "id2label.json"

    # Download and load id2label mapping from the Hugging Face Hub
    id2label_path = hf_hub_download(repo_id, filename, repo_type="dataset")
    with open(id2label_path, "r") as file:
        id2label = json.load(file)

    # Convert keys to integers
    id2label = {int(k): v for k, v in id2label.items()}
    print(id2label)

    # Load training and validation datasets from the Hugging Face Hub
    train_dataset = load_dataset("EduardoPacheco/FoodSeg103", split="train")
    val_dataset = load_dataset("EduardoPacheco/FoodSeg103", split="validation")

    # Initialize the SegmentationTrainer with loaded configuration and datasets
    trainer = SegmentationTrainer(
        train_dataset=train_dataset,
        val_dataset=val_dataset,
        id2label=id2label,
        batch_size=config["batch_size"],
        lr=config["learning_rate"],
        epochs=config["epochs"],
        save_path=config["save_path"],
        load_checkpoint=config["load_checkpoint"],
        log_dir=config["log_dir"],
    )

    # Start training the model
    trainer.train()
```
# ——init_.py

```js
from .train import SegmentationTrainer
```

# gradio_app
# app.py
```python
import gradio as gr
from gradio_app.model_inference import predict_masks

with gr.Blocks() as FoodSeg_GUI:
    # Centered title and subtitle
    gr.Markdown(
        "# **<p align='center'>Fine-Tuned Mask2Former for FoodSeg103 Semantic Segmentation</p>**"
    )
    gr.Markdown("<p align='center'>By Nima Vahdat</p>")

    # Group for input and output
    with gr.Group():
        with gr.Row():
            with gr.Column(scale=1.5):
                input_image = gr.Image(
                    type="filepath", label="Choose your image or drag and drop here:"
                )
            with gr.Column(scale=1.5):
                output_image = gr.Image(label="Mask2Former Output:")

    # Group for the run button
    with gr.Group():
        with gr.Row():
            with gr.Column(scale=1):
                start_run = gr.Button("Get the output")

    # Button click event
    start_run.click(predict_masks, inputs=input_image, outputs=output_image)

if __name__ == "__main__":
    FoodSeg_GUI.launch(share=True, debug=False)
```
# model_inferfence.py

```py
import os
import json
import random
import numpy as np
import torch
import cv2
from PIL import Image, ImageDraw, ImageFont
from transformers import Mask2FormerForUniversalSegmentation, Mask2FormerImageProcessor
from huggingface_hub import hf_hub_download
import albumentations as A


def color_palette(num_classes=150, seed=85):
    """
    Generates a consistent color palette for a given number of classes by setting a random seed.

    Args:
        num_classes (int): Number of classes/colors to generate.
        seed (int): Seed for the random number generator.

    Returns:
        list: A list of RGB values.
    """
    random.seed(seed)
    palette = []
    for _ in range(num_classes):
        color = [random.randint(0, 255) for _ in range(3)]
        palette.append(color)
    return palette


def load_model_and_processor(device):
    """
    Loads the Mask2Former model and processor from the latest checkpoint in the specified directory.

    Args:
        device (str): Device to load the model onto (e.g., 'cpu' or 'cuda').

    Returns:
        tuple: A tuple containing the loaded model and processor.
    """
    directory_path = "path_to_save_model"
    all_files = os.listdir(directory_path)
    sorted_files = sorted(all_files)
    saved_model_path = os.path.join(directory_path, sorted_files[-1])

    model = Mask2FormerForUniversalSegmentation.from_pretrained(saved_model_path).to(
        device
    )
    processor = Mask2FormerImageProcessor(
        ignore_index=0,
        reduce_labels=False,
        do_resize=False,
        do_rescale=False,
        do_normalize=False,
    )
    return model, processor


def visualize_panoptic_segmentation(
    original_image_np, segmentation_mask, segments_info, category_names
):
    """
    Visualizes the segmentation mask overlaid on the original image with category labels.

    Args:
        original_image_np (np.ndarray): The original image in NumPy array format.
        segmentation_mask (np.ndarray): The segmentation mask.
        segments_info (list): Information about the segments.
        category_names (list): List of category names corresponding to segment IDs.

    Returns:
        PIL.Image.Image: The overlayed image with segmentation mask and labels.
    """
    # Create a blank image for the segmentation mask
    segmentation_image = np.zeros_like(original_image_np)

    num_classes = len(category_names)
    palette = color_palette(num_classes)

    # Apply colors to the segmentation mask
    for segment in segments_info:
        if segment["label_id"] == 0:
            continue
        color = palette[segment["label_id"]]
        mask = segmentation_mask == segment["id"]
        segmentation_image[mask] = color

    # Overlay the segmentation mask on the original image
    alpha = 0.5  # Transparency for the overlay
    overlay_image = cv2.addWeighted(
        original_image_np, 1 - alpha, segmentation_image, alpha, 0
    )

    # Convert to PIL image for text drawing
    overlay_image_pil = Image.fromarray(overlay_image)
    draw = ImageDraw.Draw(overlay_image_pil)

    # Set up font size
    base_font_size = max(
        20, int(min(original_image_np.shape[0], original_image_np.shape[1]) * 0.015)
    )

    # Optional: Load custom font
    try:
        font = ImageFont.truetype("DejaVuSans-Bold.ttf", base_font_size)
    except IOError:
        raise RuntimeError(
            "Custom font not found. Please ensure the font file is available."
        )

    # Draw category labels on the image
    for segment in segments_info:
        label_id = segment.get("label_id")
        if label_id is not None and 0 <= label_id < len(category_names):
            category = category_names[label_id]
            mask = (segmentation_mask == segment["id"]).astype(np.uint8)
            num_labels, labels, stats, centroids = cv2.connectedComponentsWithStats(
                mask, connectivity=8
            )

            if num_labels > 1:
                largest_component = 1 + np.argmax(stats[1:, cv2.CC_STAT_AREA])
                centroid_x = int(centroids[largest_component][0])
                centroid_y = int(centroids[largest_component][1])

                # Ensure text is within image bounds
                text_position = (
                    max(0, min(centroid_x, original_image_np.shape[1] - 1)),
                    max(0, min(centroid_y, original_image_np.shape[0] - 1)),
                )
                draw.text(text_position, category, fill=(0, 0, 0), font=font)

    return overlay_image_pil


def predict_masks(input_image_path):
    """
    Predicts and visualizes segmentation masks for a given image.

    Args:
        input_image_path (str): Path to the input image.

    Returns:
        PIL.Image.Image: The image with overlaid segmentation mask and labels.
    """
    # Determine device to use
    device = "cuda" if torch.cuda.is_available() else "cpu"
    model, processor = load_model_and_processor(device)

    # Load category labels
    repo_id = "EduardoPacheco/FoodSeg103"
    filename = "id2label.json"
    id2label = json.load(
        open(hf_hub_download(repo_id, filename, repo_type="dataset"), "r")
    )
    id2label = {int(k): v for k, v in id2label.items()}

    # Load and preprocess image
    image_PIL = Image.open(input_image_path)
    original_image_np = np.array(image_PIL)

    transform = A.Compose(
        [
            A.Resize(width=512, height=512),
            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
        ]
    )

    transformed = transform(image=original_image_np)
    image = transformed["image"]

    # Convert image to C, H, W format
    image = image.transpose(2, 0, 1)

    # Process the image and get predictions
    inputs = processor([image], return_tensors="pt").to(device)

    with torch.no_grad():
        outputs = model(**inputs)

    result = processor.post_process_instance_segmentation(
        outputs, target_sizes=[image_PIL.size[::-1]]
    )[0]
    segmentation_mask = result["segmentation"].cpu().numpy()

    segments_info = result["segments_info"]
    output_result = visualize_panoptic_segmentation(
        original_image_np, segmentation_mask, segments_info, id2label
    )

    return output_result
```

# Data/_init_.py

```python
from .datasetmaker import ImageSegmentationDataset
```
# Data/datasetmaker.py

```python
import numpy as np
from torch.utils.data import Dataset


class ImageSegmentationDataset(Dataset):
    """Custom dataset for image segmentation tasks."""

    def __init__(self, dataset, transform=None, is_train=True):
        """
        Args:
            dataset (Dataset): The dataset containing image and segmentation map pairs.
            transform (callable, optional): Optional transform to be applied on a sample.
            is_train (bool, optional): Flag indicating if the dataset is for training. Defaults to True.
        """
        self.dataset = dataset
        self.is_train = is_train
        self.transform = transform

    def __len__(self):
        """Return the total number of samples in the dataset."""
        return len(self.dataset)

    def __getitem__(self, idx):
        """
        Retrieve a sample from the dataset.

        Args:
            idx (int): Index of the sample to retrieve.

        Returns:
            tuple: Transformed image, transformed segmentation map, original image, original segmentation map.
        """
        # Attempt to retrieve the image; fallback to 'image' key if 'pixel_values' key is unavailable.
        try:
            original_image = np.array(self.dataset[idx]["pixel_values"])
        except KeyError:
            original_image = np.array(self.dataset[idx]["image"])

        # Retrieve the corresponding segmentation map.
        original_segmentation_map = np.array(self.dataset[idx]["label"])

        # Apply transformations (if any) to the image and segmentation map.
        transformed = self.transform(
            image=original_image, mask=original_segmentation_map
        )
        image, segmentation_map = transformed["image"], transformed["mask"]

        # Convert image from H, W, C format to C, H, W format (required by PyTorch).
        image = image.transpose(2, 0, 1)

        return image, segmentation_map, original_image, original_segmentation_map
```
# .github/workerflowas

# jekyll-gh-pages.yml

```js
# Sample workflow for building and deploying a Jekyll site to GitHub Pages
name: Deploy Jekyll with GitHub Pages dependencies preinstalled

on:
  # Runs on pushes targeting the default branch
  push:
    branches: ["main"]

  # Allows you to run this workflow manually from the Actions tab
  workflow_dispatch:

# Sets permissions of the GITHUB_TOKEN to allow deployment to GitHub Pages
permissions:
  contents: read
  pages: write
  id-token: write

# Allow only one concurrent deployment, skipping runs queued between the run in-progress and latest queued.
# However, do NOT cancel in-progress runs as we want to allow these production deployments to complete.
concurrency:
  group: "pages"
  cancel-in-progress: false

jobs:
  # Build job
  build:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v4
      - name: Setup Pages
        uses: actions/configure-pages@v5
      - name: Build with Jekyll
        uses: actions/jekyll-build-pages@v1
        with:
          source: ./
          destination: ./_site
      - name: Upload artifact
        uses: actions/upload-pages-artifact@v3

  # Deployment job
  deploy:
    environment:
      name: github-pages
      url: ${{ steps.deployment.outputs.page_url }}
    runs-on: ubuntu-latest
    needs: build
    steps:
      - name: Deploy to GitHub Pages
        id: deployment
        uses: actions/deploy-pages@v4
```


