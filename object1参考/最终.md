I'll help you modify the code to work step-by-step in Google Colab with the FoodSeg103 dataset. Here's the complete Colab notebook:

```python
# Step 1: Install required packages
!pip install -q torch torchvision transformers datasets albumentations opencv-python Pillow matplotlib huggingface-hub evaluate pyyaml

# Step 2: Import all necessary libraries
import os
import glob
import json
import yaml
import torch
import numpy as np
from torch.utils.data import Dataset, DataLoader
from transformers import (
    Mask2FormerConfig,
    Mask2FormerForUniversalSegmentation,
    Mask2FormerImageProcessor
)
from torch.utils.tensorboard import SummaryWriter
from tqdm.auto import tqdm
import evaluate
import albumentations as A
import cv2
from PIL import Image
import matplotlib.pyplot as plt

# Step 3: Download and prepare the FoodSeg103 dataset
!wget -q https://research.larc.smu.edu.sg/downloads/datarepo/FoodSeg103.zip -P /content/datasets/
!unzip -q -P LARCdataset9947 /content/datasets/FoodSeg103.zip -d /content/datasets/FoodSeg103

# Step 4: Prepare dataset paths
root = '/content/datasets/FoodSeg103/FoodSeg103'
image_dir = os.path.join(root, 'Images/img_dir/train')
mask_dir = os.path.join(root, 'Images/ann_dir/train')

# Get all image paths and corresponding mask paths
image_paths = glob.glob(os.path.join(image_dir, '*.jpg'))
mask_paths = [os.path.join(mask_dir, os.path.basename(p).replace('.jpg', '.png')) for p in image_paths]

print(f"Found {len(image_paths)} training images")
print(f"Found {len(mask_paths)} training masks")

# Step 5: Create the dataset class
class FoodSegmentationDataset(Dataset):
    def __init__(self, image_paths, mask_paths, transform=None):
        self.image_paths = image_paths
        self.mask_paths = mask_paths
        self.transform = transform
        
    def __len__(self):
        return len(self.image_paths)
    
    def __getitem__(self, idx):
        image = np.array(Image.open(self.image_paths[idx]).convert('RGB'))
        mask = np.array(Image.open(self.mask_paths[idx]))
        
        if self.transform:
            transformed = self.transform(image=image, mask=mask)
            image = transformed['image']
            mask = transformed['mask']
            
        return image.transpose(2, 0, 1), mask, image, mask

# Step 6: Define data transformations
train_transform = A.Compose([
    A.LongestMaxSize(max_size=1333),
    A.Resize(width=512, height=512),
    A.HorizontalFlip(p=0.5),
    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])

val_transform = A.Compose([
    A.Resize(width=512, height=512),
    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])

# Step 7: Create train and validation datasets
# Split into train and validation (80/20)
split_idx = int(len(image_paths) * 0.8)
train_dataset = FoodSegmentationDataset(image_paths[:split_idx], mask_paths[:split_idx], train_transform)
val_dataset = FoodSegmentationDataset(image_paths[split_idx:], mask_paths[split_idx:], val_transform)

# Step 8: Load id2label mapping
# Download id2label.json from Hugging Face
!wget -q https://huggingface.co/datasets/EduardoPacheco/FoodSeg103/resolve/main/id2label.json -P /content/

with open('/content/id2label.json', 'r') as f:
    id2label = json.load(f)
    id2label = {int(k): v for k, v in id2label.items()}

# Step 9: Create the trainer class
class SegmentationTrainer:
    def __init__(self, train_dataset, val_dataset, id2label, batch_size=4, lr=5e-5, epochs=10):
        self.id2label = id2label
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        # Initialize processor
        self.processor = Mask2FormerImageProcessor(
            ignore_index=0,
            reduce_labels=False,
            do_resize=False,
            do_rescale=False,
            do_normalize=False,
        )
        
        # Create data loaders
        self.train_loader = DataLoader(
            train_dataset,
            batch_size=batch_size,
            shuffle=True,
            collate_fn=self.collate_fn,
        )
        self.val_loader = DataLoader(
            val_dataset,
            batch_size=batch_size,
            shuffle=False,
            collate_fn=self.collate_fn,
        )
        
        # Initialize model
        config = Mask2FormerConfig.from_pretrained("facebook/mask2former-swin-small-ade-semantic")
        config.id2label = id2label
        config.label2id = {v: k for k, v in id2label.items()}
        
        self.model = Mask2FormerForUniversalSegmentation.from_pretrained(
            "facebook/mask2former-swin-small-ade-semantic",
            config=config,
            ignore_mismatched_sizes=True,
        ).to(self.device)
        
        # Optimizer and scheduler
        self.optimizer = torch.optim.AdamW(self.model.parameters(), lr=lr, weight_decay=0.01)
        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
            self.optimizer, mode="min", factor=0.1, patience=3
        )
        
        # Metrics
        self.metric = evaluate.load("mean_iou")
        self.epochs = epochs
        
        # TensorBoard
        self.writer = SummaryWriter(log_dir='/content/logs')
        
    def collate_fn(self, batch):
        images = [item[0] for item in batch]
        segmentation_maps = [item[1] for item in batch]
        original_images = [item[2] for item in batch]
        original_segmentation_maps = [item[3] for item in batch]
        
        batch = self.processor(
            images,
            segmentation_maps=segmentation_maps,
            return_tensors="pt",
        )
        batch["original_images"] = original_images
        batch["original_segmentation_maps"] = original_segmentation_maps
        
        return batch
    
    def train(self):
        for epoch in range(self.epochs):
            self.model.train()
            epoch_loss = 0.0
            
            for batch in tqdm(self.train_loader, desc=f"Epoch {epoch+1}"):
                self.optimizer.zero_grad()
                
                outputs = self.model(
                    pixel_values=batch["pixel_values"].to(self.device),
                    mask_labels=[labels.to(self.device) for labels in batch["mask_labels"]],
                    class_labels=[labels.to(self.device) for labels in batch["class_labels"]],
                )
                
                loss = outputs.loss
                loss.backward()
                self.optimizer.step()
                
                epoch_loss += loss.item()
            
            avg_train_loss = epoch_loss / len(self.train_loader)
            self.writer.add_scalar("Loss/Train", avg_train_loss, epoch)
            print(f"Epoch {epoch+1} - Train Loss: {avg_train_loss:.4f}")
            
            # Validation
            avg_val_loss, mean_iou = self.validate()
            self.writer.add_scalar("Loss/Val", avg_val_loss, epoch)
            self.writer.add_scalar("Metric/MeanIoU", mean_iou, epoch)
            print(f"Epoch {epoch+1} - Val Loss: {avg_val_loss:.4f}, Mean IoU: {mean_iou:.4f}")
            
            # Update LR
            self.scheduler.step(avg_val_loss)
            
            # Save model checkpoint
            if epoch == 0 or avg_val_loss < getattr(self, "best_val_loss", float('inf')):
                print(f"Saving model at epoch {epoch+1}")
                os.makedirs("/content/checkpoints", exist_ok=True)
                torch.save(self.model.state_dict(), f"/content/checkpoints/epoch_{epoch+1}.pt")
                self.best_val_loss = avg_val_loss
    
    def validate(self):
        self.model.eval()
        val_loss = 0.0
        
        with torch.no_grad():
            for batch in tqdm(self.val_loader, desc="Validating"):
                outputs = self.model(
                    pixel_values=batch["pixel_values"].to(self.device),
                    mask_labels=[labels.to(self.device) for labels in batch["mask_labels"]],
                    class_labels=[labels.to(self.device) for labels in batch["class_labels"]],
                )
                val_loss += outputs.loss.item()
                
                # Post-process for evaluation
                original_images = batch["original_images"]
                target_sizes = [(img.shape[0], img.shape[1]) for img in original_images]
                predicted_maps = self.processor.post_process_semantic_segmentation(
                    outputs, target_sizes=target_sizes
                )
                
                # Compute metrics
                self.metric.add_batch(
                    references=batch["original_segmentation_maps"],
                    predictions=predicted_maps,
                )
        
        avg_val_loss = val_loss / len(self.val_loader)
        mean_iou = self.metric.compute(num_labels=len(self.id2label), ignore_index=0)["mean_iou"]
        
        return avg_val_loss, mean_iou

# Step 10: Initialize and run training
config = {
    "batch_size": 4,
    "learning_rate": 5e-5,
    "epochs": 10
}

trainer = SegmentationTrainer(
    train_dataset=train_dataset,
    val_dataset=val_dataset,
    id2label=id2label,
    batch_size=config["batch_size"],
    lr=config["learning_rate"],
    epochs=config["epochs"],
)

trainer.train()

# Step 11: Visualization function
def visualize_sample(dataset, idx):
    image, mask, orig_image, orig_mask = dataset[idx]
    
    # Convert image back to original scale for display
    image = image.transpose(1, 2, 0)
    image = (image * np.array([0.229, 0.224, 0.225])) + np.array([0.485, 0.456, 0.406])
    image = np.clip(image, 0, 1)
    
    fig, ax = plt.subplots(1, 2, figsize=(12, 6))
    ax[0].imshow(image)
    ax[0].set_title("Image")
    ax[0].axis('off')
    
    ax[1].imshow(mask, cmap='jet')
    ax[1].set_title("Mask")
    ax[1].axis('off')
    
    plt.show()

# Visualize a sample
visualize_sample(train_dataset, 0)

# Step 12: Inference function
def predict_and_visualize(model, processor, dataset, idx, device):
    model.eval()
    image, mask, orig_image, orig_mask = dataset[idx]
    
    with torch.no_grad():
        inputs = processor([image], return_tensors="pt").to(device)
        outputs = model(**inputs)
        
        # Post-process
        target_size = (orig_image.shape[0], orig_image.shape[1])
        predicted_map = processor.post_process_semantic_segmentation(
            outputs, target_sizes=[target_size]
        )[0].cpu().numpy()
    
    # Visualize
    fig, ax = plt.subplots(1, 3, figsize=(18, 6))
    ax[0].imshow(orig_image)
    ax[0].set_title("Original Image")
    ax[0].axis('off')
    
    ax[1].imshow(orig_mask, cmap='jet')
    ax[1].set_title("Ground Truth")
    ax[1].axis('off')
    
    ax[2].imshow(predicted_map, cmap='jet')
    ax[2].set_title("Prediction")
    ax[2].axis('off')
    
    plt.show()

# Load best model for inference
best_model = Mask2FormerForUniversalSegmentation.from_pretrained(
    "facebook/mask2former-swin-small-ade-semantic",
    config=trainer.model.config,
    ignore_mismatched_sizes=True,
).to(trainer.device)
best_model.load_state_dict(torch.load("/content/checkpoints/epoch_1.pt"))

# Run inference
predict_and_visualize(best_model, trainer.processor, val_dataset, 0, trainer.device)
```

This Colab notebook includes:

1. Installation of required packages
2. Dataset download and preparation
3. Dataset class implementation
4. Data transformations
5. Model initialization with FoodSeg103 configuration
6. Training loop with validation
7. Visualization of training samples
8. Inference on validation samples

Key modifications from the original code:
- Simplified the directory structure handling
- Removed dependency on Hugging Face datasets (using local files instead)
- Combined all components into a single notebook
- Added visualization functions to see results directly
- Simplified the configuration system
- Made the training process more interactive with progress bars

You can run this notebook cell by cell in Google Colab. Make sure to:
1. Use a GPU runtime (Runtime > Change runtime type > GPU)
2. Run the cells in order
3. The training will save checkpoints to `/content/checkpoints`
4. You can adjust the number of epochs, batch size, etc. in the config dictionary
