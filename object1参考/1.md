**Mini-project: general info**

- Contributes to 60% to the final assessment
- Group project (2-3 people)
- Form your group within 1 week
- Project deadline: April 26 (week 13)
- Grading:
  - we give an overall grade, you decide the weighting
  - individual grade = overall grade × group size × contribution percentage
  - need to provide a contribution statement (in tech report) to justify the weighting
  - e.g., if overall grade is 70, A and B contributed equally (i.e., each 50%), A gets 70x2x50%=70, B gets 70x2x50%=70
  - e.g., if overall grade is 70, A did 60% and B did 40%, then A gets 70x2x60%=84, B gets 70x2x40%=56

---

**Mini-project: submission**

- **Demo (10%)**
  - you run the demo and explain the designs and workflow, we do Q&A
  - keep it within 10 mins
- **Tech report (50%)**
  - describe in detail each component of your system, including datasets, model design, model training, and model evaluation (where applicable)
  - declare contributions and provide the weighting (e.g., A=50%, B=50%)
  - no more than 4 pages, double column
  - must use the provided latex template, only submit a pdf
- **Source code (40%)**
  - jupyter notebooks (containing results of training, evaluation, and visualization)
  - python files (essential functions)

---

**Mini-project: assessment criteria**

1. Clear understanding of the project objectives
2. Proper use of reference datasets and any additional data sources
3. Data preprocessing and augmentation techniques used
4. Correctness and efficiency of the algorithms and models developed
5. Innovation in approach and use of technology
6. Comprehensive testing of the system or models
7. Use of appropriate metrics for evaluation
8. Analysis and discussion of results and limitations
9. Evidence of effective collaboration
10. Clarity and organization of the demo and ability to answer questions

---

**Mini-project: FAQ**

- Do all members need to submit the files? A: Yes. Pdf + code in a zip file.
- Do all group members need to participate in demo? A: Up to you. The demo can be presented by either one member or multiple members as long as it is clear and comprehensive.
- Can I use pre-trained models? A: Only foundation models like CLIP, LLM, and Stable Diffusion. Need to train your own models like image classifiers, pose estimation models, etc.
- Can I add new functions/ideas? A: Yes, of course! We encourage innovation.
- When is the deadline for submitting the code and report? A: End of last class (on April 26). A 5% penalty will be given to late submission. Non-negotiable.


  **Project 2: SmartRecipe**

- **Overview**
  - SmartRecipe is a revolutionary app that simplifies cooking by using advanced image recognition to identify ingredients from user photos. More than just a recipe finder, it suggests personalized dishes based on available ingredients and provides detailed nutritional insights. With SmartRecipe, meal preparation becomes effortless, food waste is reduced, and users are empowered to make smart, delicious, and healthy culinary choices.

- **Objectives**
  - Food Ingredient Identification: Train a multi-label image classification model that can recognize multiple ingredients present in the given image. Reference Datasets: MAFood121, Foodseg103
  - Recipe Recommendation System: Fine-tune an LLM to learn recipe recommendations. The model should analyze the input ingredients and suggest multiple feasible recipes, providing detailed information on ingredient quantities and preparation methods. Reference Dataset: Food Recipe
  - Nutrient Analysis System: Based on the variety of ingredients in the recipe, associate their nutritional content with the recipe information. When creating the training data for recipes recommendation, this nutritional information can be integrated to provide a more comprehensive recommendation. Reference Dataset: Product Database

---

---

**迷你项目：一般信息**

- 占最终评估的60%
- 小组项目（2-3人）
- 在1周内组成小组
- 项目截止日期：4月26日（第13周）
- 评分：
  - 我们给出总体评分，你们决定权重
  - 个人评分 = 总体评分 × 小组规模 × 贡献百分比
  - 需要提供贡献声明（在技术报告中）以证明权重
  - 例如，如果总体评分是70，A和B的贡献相等（即各50%），A得70x2x50%=70，B得70x2x50%=70
  - 例如，如果总体评分是70，A贡献了60%，B贡献了40%，那么A得70x2x60%=84，B得70x2x40%=56

---

**迷你项目：提交**

- **演示（10%）**
  - 你运行演示并解释设计和工作流程，我们进行问答
  - 保持在10分钟内
- **技术报告（50%）**
  - 详细描述系统的每个组成部分，包括数据集、模型设计、模型训练和模型评估（如适用）
  - 声明贡献并提供权重（例如，A=50%，B=50%）
  - 不超过4页，双栏
  - 必须使用提供的LaTeX模板，只提交PDF
- **源代码（40%）**
  - Jupyter笔记本（包含训练、评估和可视化结果）
  - Python文件（基本功能）

---

**迷你项目：评估标准**

1. 清晰理解项目目标
2. 正确使用参考数据集和任何其他数据源
3. 使用的数据预处理和增强技术
4. 算法和模型的正确性和效率
5. 方法和使用技术的创新
6. 对系统或模型的全面测试
7. 使用适当的评估指标
8. 结果和局限性的分析和讨论
9. 有效协作的证据
10. 演示的清晰度和组织能力以及回答问题的能力

---

**迷你项目：常见问题解答**

- 所有成员都需要提交文件吗？A：是的。PDF + 代码在ZIP文件中。
- 所有小组成员都需要参与演示吗？A：由你决定。演示可以由一个成员或多个成员展示，只要清晰全面即可。
- 我可以使用预训练模型吗？A：只能使用CLIP、LLM和Stable Diffusion等基础模型。需要训练自己的模型，如图像分类器、姿态估计模型等。
- 我可以添加新功能/想法吗？A：当然可以！我们鼓励创新。
- 提交代码和报告的截止日期是什么时候？A：最后一节课结束（4月26日）。迟交将被处以5%的罚款。不可协商。

---

**Project 2: SmartRecipe**

- **Overview**
  - SmartRecipe is a revolutionary app that simplifies cooking by using advanced image recognition to identify ingredients from user photos. More than just a recipe finder, it suggests personalized dishes based on available ingredients and provides detailed nutritional insights. With SmartRecipe, meal preparation becomes effortless, food waste is reduced, and users are empowered to make smart, delicious, and healthy culinary choices.

- **Objectives**
  - Food Ingredient Identification: Train a multi-label image classification model that can recognize multiple ingredients present in the given image. Reference Datasets: MAFood121, Foodseg103
  - Recipe Recommendation System: Fine-tune an LLM to learn recipe recommendations. The model should analyze the input ingredients and suggest multiple feasible recipes, providing detailed information on ingredient quantities and preparation methods. Reference Dataset: Food Recipe
  - Nutrient Analysis System: Based on the variety of ingredients in the recipe, associate their nutritional content with the recipe information. When creating the training data for recipes recommendation, this nutritional information can be integrated to provide a more comprehensive recommendation. Reference Dataset: Product Database

---
**项目2：SmartRecipe**

- **概述**
  - SmartRecipe是一款革命性的应用程序，它通过使用先进的图像识别技术从用户照片中识别食材来简化烹饪过程。它不仅仅是一个食谱查找器，还根据可用食材建议个性化菜肴，并提供详细的营养见解。使用SmartRecipe，餐食准备变得轻松，食物浪费减少，用户能够做出明智、美味和健康的烹饪选择。

- **目标**
  - 食材识别：训练一个多标签图像分类模型，能够识别给定图像中存在的多种食材。参考数据集：MAFood121，Foodseg103
https://github.com/lannguyen0910/food-recognition

https://xiongweiwu.github.io/foodseg103.html

  - 食谱推荐系统：微调一个大型语言模型（LLM）来学习食谱推荐。该模型应分析输入食材并建议多个可行的食谱，提供关于食材数量和制备方法的详细信息。参考数据集：食品食谱

    https://huggingface.co/datasets/BhavaishKumar112/Food_Recipe

    
  - 营养分析系统：基于食谱中各种食材的多样性，将其营养内容与食谱信息关联起来。在创建食谱推荐的训练数据时，可以将这些营养信息整合起来，以提供更全面的建议。参考数据集：产品数据库

    https://huggingface.co/datasets/openfoodfacts/product-database
# Object1

Food Ingredient Identification: Train a multi-label image classification model that can recognize multiple ingredients present in the given image. Reference Datasets: MAFood121, Foodseg103

```python
### SmartRecipe: Food Ingredient Identification
# This notebook implements multi-label classification of ingredients in food images
# using the FoodSeg103 and MAFood121 datasets. It is designed to run on Google Colab.

# 1. Install and import dependencies
!pip install --quiet kaggle torch torchvision pillow matplotlib tqdm

import os
import glob
import numpy as np
from PIL import Image
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms, models
from sklearn.model_selection import train_test_split
from tqdm import tqdm
```

```python
# 4. Download FoodSeg103 dataset
!wget -q https://research.larc.smu.edu.sg/downloads/datarepo/FoodSeg103.zip -P /content/datasets/
!unzip -q -P LARCdataset9947 /content/datasets/FoodSeg103.zip -d /content/datasets/FoodSeg103

```

```python
# 5. Prepare file lists
image_dir = '/content/datasets/FoodSeg103/images'
mask_dir  = '/content/datasets/FoodSeg103/masks'
image_paths = glob.glob(os.path.join(image_dir, '*.jpg'))
mask_paths  = [os.path.join(mask_dir, os.path.basename(p).replace('.jpg', '.png')) for p in image_paths]
```
```python
# 5. Automatically detect image and mask directories
root = '/content/datasets/FoodSeg103'
image_dirs = []
mask_dirs = []
for dirpath, dirnames, filenames in os.walk(root):
    if any(f.lower().endswith('.jpg') for f in filenames):
        image_dirs.append(dirpath)
    if any(f.lower().endswith('.png') for f in filenames):
        mask_dirs.append(dirpath)
if not image_dirs or not mask_dirs:
    raise RuntimeError(f"No image or mask directory found under {root}")
# Use first found directories (adjust index if needed)
image_dir = image_dirs[0]
mask_dir  = mask_dirs[0]
print(f"Using images from: {image_dir}")
print(f"Using masks from:  {mask_dir}")
```
results:
```js
Using images from: /content/datasets/FoodSeg103/FoodSeg103/Images/img_dir/test
Using masks from:  /content/datasets/FoodSeg103/FoodSeg103/Images/ann_dir/test
```

```js
# 6. Prepare paired lists of images and masks
all_images = glob.glob(os.path.join(image_dir, '*.jpg'))
image_paths = []
mask_paths  = []
for img_path in all_images:
    mask_path = os.path.join(mask_dir, os.path.basename(img_path).replace('.jpg', '.png'))
    if os.path.exists(mask_path):
        image_paths.append(img_path)
        mask_paths.append(mask_path)
print(f"Found {len(image_paths)} paired image-mask files.")

if len(image_paths) == 0:
    raise RuntimeError("No image-mask pairs found. Please check your directory structure.")
```
results:
```js
Found 2135 paired image-mask files.
```

```js
# 7. Determine number of ingredient classes
sample_mask = np.array(Image.open(mask_paths[0]))
num_classes = int(sample_mask.max()) + 1
print(f"Detected {num_classes} ingredient classes.")
```
results:
```js
Detected 86 ingredient classes.
```
```js
# 8. Dataset and DataLoader
class FoodIngredientDataset(Dataset):
    def __init__(self, img_paths, msk_paths, num_classes, transform=None):
        self.img_paths = img_paths
        self.msk_paths = msk_paths
        self.num_classes = num_classes
        self.transform = transform

    def __len__(self):
        return len(self.img_paths)

    def __getitem__(self, idx):
        img = Image.open(self.img_paths[idx]).convert('RGB')
        mask = np.array(Image.open(self.msk_paths[idx]))
        labels = np.unique(mask)
        multi_hot = torch.zeros(self.num_classes, dtype=torch.float32)
        for c in labels:
            if c < self.num_classes:
                multi_hot[c] = 1.0
        if self.transform:
            img = self.transform(img)
        return img, multi_hot
```

```js
# 9. Transforms and DataLoader setup
data_transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])
train_imgs, val_imgs, train_masks, val_masks = train_test_split(
    image_paths, mask_paths, test_size=0.2, random_state=42
)
train_dataset = FoodIngredientDataset(train_imgs, train_masks, num_classes, transform=data_transform)
val_dataset   = FoodIngredientDataset(val_imgs,   val_masks,   num_classes, transform=data_transform)
train_loader  = DataLoader(train_dataset, batch_size=32, shuffle=True,  num_workers=2)
val_loader    = DataLoader(val_dataset,   batch_size=32, shuffle=False, num_workers=2)

```

```js
# 10. Model definition
model = models.resnet50(pretrained=True)
model.fc = nn.Linear(model.fc.in_features, num_classes)

```

results:

```js
/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
Downloading: "https://download.pytorch.org/models/resnet50-0676ba61.pth" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth
100%|██████████| 97.8M/97.8M [00:01<00:00, 94.0MB/s]
```
```js
# Enable CUDA only if available
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = model.to(device)
```

```js
# 11. Loss and optimizer
criterion = nn.BCEWithLogitsLoss()
optimizer = optim.Adam(model.parameters(), lr=1e-4)
```

```js
# 12. Training loop
num_epochs = 10
for epoch in range(num_epochs):
    model.train()
    running_loss = 0.0
    for imgs, labels in tqdm(train_loader, desc=f"Epoch {epoch+1}/{num_epochs} - Training"):
        imgs, labels = imgs.to(device), labels.to(device)
        optimizer.zero_grad()
        outputs = model(imgs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
        running_loss += loss.item() * imgs.size(0)
    epoch_loss = running_loss / len(train_loader.dataset)
    print(f"Epoch {epoch+1} Training Loss: {epoch_loss:.4f}")

    model.eval()
    val_loss = 0.0
    with torch.no_grad():
        for imgs, labels in tqdm(val_loader, desc="Validation"):
            imgs, labels = imgs.to(device), labels.to(device)
            outputs = model(imgs)
            loss = criterion(outputs, labels)
            val_loss += loss.item() * imgs.size(0)
    val_loss /= len(val_loader.dataset)
    print(f"Epoch {epoch+1} Validation Loss: {val_loss:.4f}\n")

```
results:
```js
Epoch 1/10 - Training: 100%|██████████| 54/54 [19:17<00:00, 21.43s/it]
Epoch 1 Training Loss: 0.2372
Validation: 100%|██████████| 14/14 [01:27<00:00,  6.22s/it]
Epoch 1 Validation Loss: 0.1140

Epoch 2/10 - Training: 100%|██████████| 54/54 [18:47<00:00, 20.87s/it]
Epoch 2 Training Loss: 0.1006
Validation: 100%|██████████| 14/14 [01:28<00:00,  6.33s/it]
Epoch 2 Validation Loss: 0.1008

Epoch 3/10 - Training: 100%|██████████| 54/54 [19:25<00:00, 21.59s/it]
Epoch 3 Training Loss: 0.0816
Validation: 100%|██████████| 14/14 [01:25<00:00,  6.09s/it]
Epoch 3 Validation Loss: 0.0967

Epoch 4/10 - Training: 100%|██████████| 54/54 [19:07<00:00, 21.26s/it]
Epoch 4 Training Loss: 0.0659
Validation: 100%|██████████| 14/14 [01:27<00:00,  6.24s/it]
Epoch 4 Validation Loss: 0.0964

Epoch 5/10 - Training: 100%|██████████| 54/54 [19:03<00:00, 21.18s/it]
Epoch 5 Training Loss: 0.0532
Validation: 100%|██████████| 14/14 [01:25<00:00,  6.12s/it]
Epoch 5 Validation Loss: 0.0955

Epoch 6/10 - Training: 100%|██████████| 54/54 [18:20<00:00, 20.38s/it]
Epoch 6 Training Loss: 0.0433
Validation: 100%|██████████| 14/14 [01:23<00:00,  5.95s/it]
Epoch 6 Validation Loss: 0.0967

Epoch 7/10 - Training: 100%|██████████| 54/54 [18:26<00:00, 20.49s/it]
Epoch 7 Training Loss: 0.0358
Validation: 100%|██████████| 14/14 [01:28<00:00,  6.32s/it]
Epoch 7 Validation Loss: 0.0968

Epoch 8/10 - Training: 100%|██████████| 54/54 [18:47<00:00, 20.88s/it]
Epoch 8 Training Loss: 0.0290
Validation: 100%|██████████| 14/14 [01:27<00:00,  6.28s/it]
Epoch 8 Validation Loss: 0.0964

Epoch 9/10 - Training: 100%|██████████| 54/54 [18:50<00:00, 20.94s/it]
Epoch 9 Training Loss: 0.0242
Validation: 100%|██████████| 14/14 [01:26<00:00,  6.20s/it]
Epoch 9 Validation Loss: 0.0968

Epoch 10/10 - Training: 100%|██████████| 54/54 [19:24<00:00, 21.57s/it]
Epoch 10 Training Loss: 0.0204
Validation: 100%|██████████| 14/14 [01:27<00:00,  6.24s/it]Epoch 10 Validation Loss: 0.0981

```

```js
# 13. Save model
os.makedirs('/content/models', exist_ok=True)
torch.save(model.state_dict(), '/content/models/ingredient_classifier.pth')
print("Model training complete and saved to /content/models/ingredient_classifier.pth")
```
results:
```js
Model training complete and saved to /content/models/ingredient_classifier.pth
```

```js
class_names = [
    "background",
    "candy", "egg tart", "french fries", "chocolate", "biscuit", "popcorn", "pudding", "ice cream", "cheese butter",
    "cake", "wine", "milkshake", "coffee", "juice", "milk", "tea", "almond", "red beans", "cashew",
    "dried cranberries", "soy", "walnut", "peanut", "egg", "apple", "date", "apricot", "avocado", "banana",
    "strawberry", "cherry", "blueberry", "raspberry", "mango", "olives", "peach", "lemon", "pear", "fig",
    "pineapple", "grape", "kiwi", "melon", "orange", "watermelon", "steak", "pork", "chicken duck", "sausage",
    "fried meat", "lamb", "sauce", "crab", "fish", "shellfish", "shrimp", "soup", "bread", "corn", "hamburg",
    "pizza", "hanamaki baozi", "wonton dumplings", "pasta", "noodles", "rice", "pie", "tofu", "eggplant",
    "potato", "garlic", "cauliflower", "tomato", "kelp", "seaweed", "spring onion", "rape", "ginger", "okra",
    "lettuce", "pumpkin", "cucumber", "white radish", "carrot", "asparagus", "bamboo shoots", "broccoli",
    "celery stick", "cilantro mint", "snow peas", "cabbage", "bean sprouts", "onion", "pepper", "green beans",
    "French beans", "king oyster mushroom", "shiitake", "enoki mushroom", "oyster mushroom", "white button mushroom",
    "salad", "other ingredients"
]
```


```js
import matplotlib.pyplot as plt
import os

# 定义标签解码器
def decode_labels(pred, class_names, threshold=0.5):
    return [class_names[i] for i, p in enumerate(pred) if p >= threshold]

# 创建输出文件夹
os.makedirs("/content/results", exist_ok=True)

# 可视化前5张图像
model.eval()
with torch.no_grad():
    for batch_idx, (imgs, labels) in enumerate(val_loader):
        imgs = imgs.to(device)
        outputs = model(imgs)
        preds = torch.sigmoid(outputs).detach().cpu().numpy()

        for i in range(min(10, imgs.size(0))):
            img = imgs[i].cpu().permute(1, 2, 0).numpy()
            img = img * [0.229, 0.224, 0.225] + [0.485, 0.456, 0.406]  # 反标准化
            img = np.clip(img, 0, 1)

            pred_labels = decode_labels(preds[i], class_names)
            true_labels = decode_labels(labels[i].numpy(), class_names)

            plt.figure(figsize=(5, 5))
            plt.imshow(img)
            plt.title(f"Predicted: {', '.join(pred_labels)}\nTrue: {', '.join(true_labels)}", fontsize=10)
            plt.axis('off')
            plt.tight_layout()
            plt.savefig(f"/content/results/prediction_{batch_idx}_{i}.png")
            plt.close()
        break  # 只保存第一个 batch 的预测
print("✅ Results are saved in /content/results/")
```


```js
Results are saved in /content/results/
```
```js
from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score

def evaluate_model(model, dataloader, threshold=0.5):
    model.eval()
    all_preds = []
    all_labels = []

    with torch.no_grad():
        for imgs, labels in tqdm(dataloader, desc="Evaluating"):
            imgs = imgs.to(device)
            outputs = model(imgs)
            preds = torch.sigmoid(outputs).cpu().numpy()
            labels = labels.numpy()

            all_preds.append(preds)
            all_labels.append(labels)

    all_preds = np.vstack(all_preds)
    all_labels = np.vstack(all_labels)

    binary_preds = (all_preds >= threshold).astype(int)

    precision = precision_score(all_labels, binary_preds, average='micro')
    recall = recall_score(all_labels, binary_preds, average='micro')
    f1 = f1_score(all_labels, binary_preds, average='micro')

    try:
        auc = roc_auc_score(all_labels, all_preds, average='micro')
    except:
        auc = 'N/A (AUC not defined for this batch)'

    print("🔍 Evaluation Results:")
    print(f"Precision: {precision:.4f}")
    print(f"Recall:    {recall:.4f}")
    print(f"F1 Score:  {f1:.4f}")
    print(f"AUC:       {auc}")

# 👉 使用方式：
evaluate_model(model, val_loader)
```

```js
Evaluating: 100%|██████████| 14/14 [01:31<00:00,  6.53s/it]🔍 Evaluation Results:
Precision: 0.8299
Recall:    0.4466
F1 Score:  0.5807
AUC:       0.9331802396996636
```



