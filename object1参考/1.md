**Mini-project: general info**

- Contributes to 60% to the final assessment
- Group project (2-3 people)
- Form your group within 1 week
- Project deadline: April 26 (week 13)
- Grading:
  - we give an overall grade, you decide the weighting
  - individual grade = overall grade Ã— group size Ã— contribution percentage
  - need to provide a contribution statement (in tech report) to justify the weighting
  - e.g., if overall grade is 70, A and B contributed equally (i.e., each 50%), A gets 70x2x50%=70, B gets 70x2x50%=70
  - e.g., if overall grade is 70, A did 60% and B did 40%, then A gets 70x2x60%=84, B gets 70x2x40%=56

---

**Mini-project: submission**

- **Demo (10%)**
  - you run the demo and explain the designs and workflow, we do Q&A
  - keep it within 10 mins
- **Tech report (50%)**
  - describe in detail each component of your system, including datasets, model design, model training, and model evaluation (where applicable)
  - declare contributions and provide the weighting (e.g., A=50%, B=50%)
  - no more than 4 pages, double column
  - must use the provided latex template, only submit a pdf
- **Source code (40%)**
  - jupyter notebooks (containing results of training, evaluation, and visualization)
  - python files (essential functions)

---

**Mini-project: assessment criteria**

1. Clear understanding of the project objectives
2. Proper use of reference datasets and any additional data sources
3. Data preprocessing and augmentation techniques used
4. Correctness and efficiency of the algorithms and models developed
5. Innovation in approach and use of technology
6. Comprehensive testing of the system or models
7. Use of appropriate metrics for evaluation
8. Analysis and discussion of results and limitations
9. Evidence of effective collaboration
10. Clarity and organization of the demo and ability to answer questions

---

**Mini-project: FAQ**

- Do all members need to submit the files? A: Yes. Pdf + code in a zip file.
- Do all group members need to participate in demo? A: Up to you. The demo can be presented by either one member or multiple members as long as it is clear and comprehensive.
- Can I use pre-trained models? A: Only foundation models like CLIP, LLM, and Stable Diffusion. Need to train your own models like image classifiers, pose estimation models, etc.
- Can I add new functions/ideas? A: Yes, of course! We encourage innovation.
- When is the deadline for submitting the code and report? A: End of last class (on April 26). A 5% penalty will be given to late submission. Non-negotiable.


  **Project 2: SmartRecipe**

- **Overview**
  - SmartRecipe is a revolutionary app that simplifies cooking by using advanced image recognition to identify ingredients from user photos. More than just a recipe finder, it suggests personalized dishes based on available ingredients and provides detailed nutritional insights. With SmartRecipe, meal preparation becomes effortless, food waste is reduced, and users are empowered to make smart, delicious, and healthy culinary choices.

- **Objectives**
  - Food Ingredient Identification: Train a multi-label image classification model that can recognize multiple ingredients present in the given image. Reference Datasets: MAFood121, Foodseg103
  - Recipe Recommendation System: Fine-tune an LLM to learn recipe recommendations. The model should analyze the input ingredients and suggest multiple feasible recipes, providing detailed information on ingredient quantities and preparation methods. Reference Dataset: Food Recipe
  - Nutrient Analysis System: Based on the variety of ingredients in the recipe, associate their nutritional content with the recipe information. When creating the training data for recipes recommendation, this nutritional information can be integrated to provide a more comprehensive recommendation. Reference Dataset: Product Database

---

---

**è¿·ä½ é¡¹ç›®ï¼šä¸€èˆ¬ä¿¡æ¯**

- å æœ€ç»ˆè¯„ä¼°çš„60%
- å°ç»„é¡¹ç›®ï¼ˆ2-3äººï¼‰
- åœ¨1å‘¨å†…ç»„æˆå°ç»„
- é¡¹ç›®æˆªæ­¢æ—¥æœŸï¼š4æœˆ26æ—¥ï¼ˆç¬¬13å‘¨ï¼‰
- è¯„åˆ†ï¼š
  - æˆ‘ä»¬ç»™å‡ºæ€»ä½“è¯„åˆ†ï¼Œä½ ä»¬å†³å®šæƒé‡
  - ä¸ªäººè¯„åˆ† = æ€»ä½“è¯„åˆ† Ã— å°ç»„è§„æ¨¡ Ã— è´¡çŒ®ç™¾åˆ†æ¯”
  - éœ€è¦æä¾›è´¡çŒ®å£°æ˜ï¼ˆåœ¨æŠ€æœ¯æŠ¥å‘Šä¸­ï¼‰ä»¥è¯æ˜æƒé‡
  - ä¾‹å¦‚ï¼Œå¦‚æœæ€»ä½“è¯„åˆ†æ˜¯70ï¼ŒAå’ŒBçš„è´¡çŒ®ç›¸ç­‰ï¼ˆå³å„50%ï¼‰ï¼ŒAå¾—70x2x50%=70ï¼ŒBå¾—70x2x50%=70
  - ä¾‹å¦‚ï¼Œå¦‚æœæ€»ä½“è¯„åˆ†æ˜¯70ï¼ŒAè´¡çŒ®äº†60%ï¼ŒBè´¡çŒ®äº†40%ï¼Œé‚£ä¹ˆAå¾—70x2x60%=84ï¼ŒBå¾—70x2x40%=56

---

**è¿·ä½ é¡¹ç›®ï¼šæäº¤**

- **æ¼”ç¤ºï¼ˆ10%ï¼‰**
  - ä½ è¿è¡Œæ¼”ç¤ºå¹¶è§£é‡Šè®¾è®¡å’Œå·¥ä½œæµç¨‹ï¼Œæˆ‘ä»¬è¿›è¡Œé—®ç­”
  - ä¿æŒåœ¨10åˆ†é’Ÿå†…
- **æŠ€æœ¯æŠ¥å‘Šï¼ˆ50%ï¼‰**
  - è¯¦ç»†æè¿°ç³»ç»Ÿçš„æ¯ä¸ªç»„æˆéƒ¨åˆ†ï¼ŒåŒ…æ‹¬æ•°æ®é›†ã€æ¨¡å‹è®¾è®¡ã€æ¨¡å‹è®­ç»ƒå’Œæ¨¡å‹è¯„ä¼°ï¼ˆå¦‚é€‚ç”¨ï¼‰
  - å£°æ˜è´¡çŒ®å¹¶æä¾›æƒé‡ï¼ˆä¾‹å¦‚ï¼ŒA=50%ï¼ŒB=50%ï¼‰
  - ä¸è¶…è¿‡4é¡µï¼ŒåŒæ 
  - å¿…é¡»ä½¿ç”¨æä¾›çš„LaTeXæ¨¡æ¿ï¼Œåªæäº¤PDF
- **æºä»£ç ï¼ˆ40%ï¼‰**
  - Jupyterç¬”è®°æœ¬ï¼ˆåŒ…å«è®­ç»ƒã€è¯„ä¼°å’Œå¯è§†åŒ–ç»“æœï¼‰
  - Pythonæ–‡ä»¶ï¼ˆåŸºæœ¬åŠŸèƒ½ï¼‰

---

**è¿·ä½ é¡¹ç›®ï¼šè¯„ä¼°æ ‡å‡†**

1. æ¸…æ™°ç†è§£é¡¹ç›®ç›®æ ‡
2. æ­£ç¡®ä½¿ç”¨å‚è€ƒæ•°æ®é›†å’Œä»»ä½•å…¶ä»–æ•°æ®æº
3. ä½¿ç”¨çš„æ•°æ®é¢„å¤„ç†å’Œå¢å¼ºæŠ€æœ¯
4. ç®—æ³•å’Œæ¨¡å‹çš„æ­£ç¡®æ€§å’Œæ•ˆç‡
5. æ–¹æ³•å’Œä½¿ç”¨æŠ€æœ¯çš„åˆ›æ–°
6. å¯¹ç³»ç»Ÿæˆ–æ¨¡å‹çš„å…¨é¢æµ‹è¯•
7. ä½¿ç”¨é€‚å½“çš„è¯„ä¼°æŒ‡æ ‡
8. ç»“æœå’Œå±€é™æ€§çš„åˆ†æå’Œè®¨è®º
9. æœ‰æ•ˆåä½œçš„è¯æ®
10. æ¼”ç¤ºçš„æ¸…æ™°åº¦å’Œç»„ç»‡èƒ½åŠ›ä»¥åŠå›ç­”é—®é¢˜çš„èƒ½åŠ›

---

**è¿·ä½ é¡¹ç›®ï¼šå¸¸è§é—®é¢˜è§£ç­”**

- æ‰€æœ‰æˆå‘˜éƒ½éœ€è¦æäº¤æ–‡ä»¶å—ï¼ŸAï¼šæ˜¯çš„ã€‚PDF + ä»£ç åœ¨ZIPæ–‡ä»¶ä¸­ã€‚
- æ‰€æœ‰å°ç»„æˆå‘˜éƒ½éœ€è¦å‚ä¸æ¼”ç¤ºå—ï¼ŸAï¼šç”±ä½ å†³å®šã€‚æ¼”ç¤ºå¯ä»¥ç”±ä¸€ä¸ªæˆå‘˜æˆ–å¤šä¸ªæˆå‘˜å±•ç¤ºï¼Œåªè¦æ¸…æ™°å…¨é¢å³å¯ã€‚
- æˆ‘å¯ä»¥ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹å—ï¼ŸAï¼šåªèƒ½ä½¿ç”¨CLIPã€LLMå’ŒStable Diffusionç­‰åŸºç¡€æ¨¡å‹ã€‚éœ€è¦è®­ç»ƒè‡ªå·±çš„æ¨¡å‹ï¼Œå¦‚å›¾åƒåˆ†ç±»å™¨ã€å§¿æ€ä¼°è®¡æ¨¡å‹ç­‰ã€‚
- æˆ‘å¯ä»¥æ·»åŠ æ–°åŠŸèƒ½/æƒ³æ³•å—ï¼ŸAï¼šå½“ç„¶å¯ä»¥ï¼æˆ‘ä»¬é¼“åŠ±åˆ›æ–°ã€‚
- æäº¤ä»£ç å’ŒæŠ¥å‘Šçš„æˆªæ­¢æ—¥æœŸæ˜¯ä»€ä¹ˆæ—¶å€™ï¼ŸAï¼šæœ€åä¸€èŠ‚è¯¾ç»“æŸï¼ˆ4æœˆ26æ—¥ï¼‰ã€‚è¿Ÿäº¤å°†è¢«å¤„ä»¥5%çš„ç½šæ¬¾ã€‚ä¸å¯åå•†ã€‚

---

**Project 2: SmartRecipe**

- **Overview**
  - SmartRecipe is a revolutionary app that simplifies cooking by using advanced image recognition to identify ingredients from user photos. More than just a recipe finder, it suggests personalized dishes based on available ingredients and provides detailed nutritional insights. With SmartRecipe, meal preparation becomes effortless, food waste is reduced, and users are empowered to make smart, delicious, and healthy culinary choices.

- **Objectives**
  - Food Ingredient Identification: Train a multi-label image classification model that can recognize multiple ingredients present in the given image. Reference Datasets: MAFood121, Foodseg103
  - Recipe Recommendation System: Fine-tune an LLM to learn recipe recommendations. The model should analyze the input ingredients and suggest multiple feasible recipes, providing detailed information on ingredient quantities and preparation methods. Reference Dataset: Food Recipe
  - Nutrient Analysis System: Based on the variety of ingredients in the recipe, associate their nutritional content with the recipe information. When creating the training data for recipes recommendation, this nutritional information can be integrated to provide a more comprehensive recommendation. Reference Dataset: Product Database

---
**é¡¹ç›®2ï¼šSmartRecipe**

- **æ¦‚è¿°**
  - SmartRecipeæ˜¯ä¸€æ¬¾é©å‘½æ€§çš„åº”ç”¨ç¨‹åºï¼Œå®ƒé€šè¿‡ä½¿ç”¨å…ˆè¿›çš„å›¾åƒè¯†åˆ«æŠ€æœ¯ä»ç”¨æˆ·ç…§ç‰‡ä¸­è¯†åˆ«é£Ÿææ¥ç®€åŒ–çƒ¹é¥ªè¿‡ç¨‹ã€‚å®ƒä¸ä»…ä»…æ˜¯ä¸€ä¸ªé£Ÿè°±æŸ¥æ‰¾å™¨ï¼Œè¿˜æ ¹æ®å¯ç”¨é£Ÿæå»ºè®®ä¸ªæ€§åŒ–èœè‚´ï¼Œå¹¶æä¾›è¯¦ç»†çš„è¥å…»è§è§£ã€‚ä½¿ç”¨SmartRecipeï¼Œé¤é£Ÿå‡†å¤‡å˜å¾—è½»æ¾ï¼Œé£Ÿç‰©æµªè´¹å‡å°‘ï¼Œç”¨æˆ·èƒ½å¤Ÿåšå‡ºæ˜æ™ºã€ç¾å‘³å’Œå¥åº·çš„çƒ¹é¥ªé€‰æ‹©ã€‚

- **ç›®æ ‡**
  - é£Ÿæè¯†åˆ«ï¼šè®­ç»ƒä¸€ä¸ªå¤šæ ‡ç­¾å›¾åƒåˆ†ç±»æ¨¡å‹ï¼Œèƒ½å¤Ÿè¯†åˆ«ç»™å®šå›¾åƒä¸­å­˜åœ¨çš„å¤šç§é£Ÿæã€‚å‚è€ƒæ•°æ®é›†ï¼šMAFood121ï¼ŒFoodseg103
https://github.com/lannguyen0910/food-recognition

https://xiongweiwu.github.io/foodseg103.html

  - é£Ÿè°±æ¨èç³»ç»Ÿï¼šå¾®è°ƒä¸€ä¸ªå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¥å­¦ä¹ é£Ÿè°±æ¨èã€‚è¯¥æ¨¡å‹åº”åˆ†æè¾“å…¥é£Ÿæå¹¶å»ºè®®å¤šä¸ªå¯è¡Œçš„é£Ÿè°±ï¼Œæä¾›å…³äºé£Ÿææ•°é‡å’Œåˆ¶å¤‡æ–¹æ³•çš„è¯¦ç»†ä¿¡æ¯ã€‚å‚è€ƒæ•°æ®é›†ï¼šé£Ÿå“é£Ÿè°±

    https://huggingface.co/datasets/BhavaishKumar112/Food_Recipe

    
  - è¥å…»åˆ†æç³»ç»Ÿï¼šåŸºäºé£Ÿè°±ä¸­å„ç§é£Ÿæçš„å¤šæ ·æ€§ï¼Œå°†å…¶è¥å…»å†…å®¹ä¸é£Ÿè°±ä¿¡æ¯å…³è”èµ·æ¥ã€‚åœ¨åˆ›å»ºé£Ÿè°±æ¨èçš„è®­ç»ƒæ•°æ®æ—¶ï¼Œå¯ä»¥å°†è¿™äº›è¥å…»ä¿¡æ¯æ•´åˆèµ·æ¥ï¼Œä»¥æä¾›æ›´å…¨é¢çš„å»ºè®®ã€‚å‚è€ƒæ•°æ®é›†ï¼šäº§å“æ•°æ®åº“

    https://huggingface.co/datasets/openfoodfacts/product-database
# Object1

Food Ingredient Identification: Train a multi-label image classification model that can recognize multiple ingredients present in the given image. Reference Datasets: MAFood121, Foodseg103

```python
### SmartRecipe: Food Ingredient Identification
# This notebook implements multi-label classification of ingredients in food images
# using the FoodSeg103 and MAFood121 datasets. It is designed to run on Google Colab.

# 1. Install and import dependencies
!pip install --quiet kaggle torch torchvision pillow matplotlib tqdm

import os
import glob
import numpy as np
from PIL import Image
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms, models
from sklearn.model_selection import train_test_split
from tqdm import tqdm
```

```python
# 4. Download FoodSeg103 dataset
!wget -q https://research.larc.smu.edu.sg/downloads/datarepo/FoodSeg103.zip -P /content/datasets/
!unzip -q -P LARCdataset9947 /content/datasets/FoodSeg103.zip -d /content/datasets/FoodSeg103

```

```python
# 5. Prepare file lists
image_dir = '/content/datasets/FoodSeg103/images'
mask_dir  = '/content/datasets/FoodSeg103/masks'
image_paths = glob.glob(os.path.join(image_dir, '*.jpg'))
mask_paths  = [os.path.join(mask_dir, os.path.basename(p).replace('.jpg', '.png')) for p in image_paths]
```
```python
# 5. Automatically detect image and mask directories
root = '/content/datasets/FoodSeg103'
image_dirs = []
mask_dirs = []
for dirpath, dirnames, filenames in os.walk(root):
    if any(f.lower().endswith('.jpg') for f in filenames):
        image_dirs.append(dirpath)
    if any(f.lower().endswith('.png') for f in filenames):
        mask_dirs.append(dirpath)
if not image_dirs or not mask_dirs:
    raise RuntimeError(f"No image or mask directory found under {root}")
# Use first found directories (adjust index if needed)
image_dir = image_dirs[0]
mask_dir  = mask_dirs[0]
print(f"Using images from: {image_dir}")
print(f"Using masks from:  {mask_dir}")
```
results:
```js
Using images from: /content/datasets/FoodSeg103/FoodSeg103/Images/img_dir/test
Using masks from:  /content/datasets/FoodSeg103/FoodSeg103/Images/ann_dir/test
```

```js
# 6. Prepare paired lists of images and masks
all_images = glob.glob(os.path.join(image_dir, '*.jpg'))
image_paths = []
mask_paths  = []
for img_path in all_images:
    mask_path = os.path.join(mask_dir, os.path.basename(img_path).replace('.jpg', '.png'))
    if os.path.exists(mask_path):
        image_paths.append(img_path)
        mask_paths.append(mask_path)
print(f"Found {len(image_paths)} paired image-mask files.")

if len(image_paths) == 0:
    raise RuntimeError("No image-mask pairs found. Please check your directory structure.")
```
results:
```js
Found 2135 paired image-mask files.
```

```js
# 7. Determine number of ingredient classes
sample_mask = np.array(Image.open(mask_paths[0]))
num_classes = int(sample_mask.max()) + 1
print(f"Detected {num_classes} ingredient classes.")
```
results:
```js
Detected 86 ingredient classes.
```
```js
# 8. Dataset and DataLoader
class FoodIngredientDataset(Dataset):
    def __init__(self, img_paths, msk_paths, num_classes, transform=None):
        self.img_paths = img_paths
        self.msk_paths = msk_paths
        self.num_classes = num_classes
        self.transform = transform

    def __len__(self):
        return len(self.img_paths)

    def __getitem__(self, idx):
        img = Image.open(self.img_paths[idx]).convert('RGB')
        mask = np.array(Image.open(self.msk_paths[idx]))
        labels = np.unique(mask)
        multi_hot = torch.zeros(self.num_classes, dtype=torch.float32)
        for c in labels:
            if c < self.num_classes:
                multi_hot[c] = 1.0
        if self.transform:
            img = self.transform(img)
        return img, multi_hot
```

```js
# 9. Transforms and DataLoader setup
data_transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])
train_imgs, val_imgs, train_masks, val_masks = train_test_split(
    image_paths, mask_paths, test_size=0.2, random_state=42
)
train_dataset = FoodIngredientDataset(train_imgs, train_masks, num_classes, transform=data_transform)
val_dataset   = FoodIngredientDataset(val_imgs,   val_masks,   num_classes, transform=data_transform)
train_loader  = DataLoader(train_dataset, batch_size=32, shuffle=True,  num_workers=2)
val_loader    = DataLoader(val_dataset,   batch_size=32, shuffle=False, num_workers=2)

```

```js
# 10. Model definition
model = models.resnet50(pretrained=True)
model.fc = nn.Linear(model.fc.in_features, num_classes)

```

results:

```js
/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
Downloading: "https://download.pytorch.org/models/resnet50-0676ba61.pth" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 97.8M/97.8M [00:01<00:00, 94.0MB/s]
```
```js
# Enable CUDA only if available
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = model.to(device)
```

```js
# 11. Loss and optimizer
criterion = nn.BCEWithLogitsLoss()
optimizer = optim.Adam(model.parameters(), lr=1e-4)
```

```js
# 12. Training loop
num_epochs = 10
for epoch in range(num_epochs):
    model.train()
    running_loss = 0.0
    for imgs, labels in tqdm(train_loader, desc=f"Epoch {epoch+1}/{num_epochs} - Training"):
        imgs, labels = imgs.to(device), labels.to(device)
        optimizer.zero_grad()
        outputs = model(imgs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
        running_loss += loss.item() * imgs.size(0)
    epoch_loss = running_loss / len(train_loader.dataset)
    print(f"Epoch {epoch+1} Training Loss: {epoch_loss:.4f}")

    model.eval()
    val_loss = 0.0
    with torch.no_grad():
        for imgs, labels in tqdm(val_loader, desc="Validation"):
            imgs, labels = imgs.to(device), labels.to(device)
            outputs = model(imgs)
            loss = criterion(outputs, labels)
            val_loss += loss.item() * imgs.size(0)
    val_loss /= len(val_loader.dataset)
    print(f"Epoch {epoch+1} Validation Loss: {val_loss:.4f}\n")

```
results:
```js
Epoch 1/10 - Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 54/54 [19:17<00:00, 21.43s/it]
Epoch 1 Training Loss: 0.2372
Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14/14 [01:27<00:00,  6.22s/it]
Epoch 1 Validation Loss: 0.1140

Epoch 2/10 - Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 54/54 [18:47<00:00, 20.87s/it]
Epoch 2 Training Loss: 0.1006
Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14/14 [01:28<00:00,  6.33s/it]
Epoch 2 Validation Loss: 0.1008

Epoch 3/10 - Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 54/54 [19:25<00:00, 21.59s/it]
Epoch 3 Training Loss: 0.0816
Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14/14 [01:25<00:00,  6.09s/it]
Epoch 3 Validation Loss: 0.0967

Epoch 4/10 - Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 54/54 [19:07<00:00, 21.26s/it]
Epoch 4 Training Loss: 0.0659
Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14/14 [01:27<00:00,  6.24s/it]
Epoch 4 Validation Loss: 0.0964

Epoch 5/10 - Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 54/54 [19:03<00:00, 21.18s/it]
Epoch 5 Training Loss: 0.0532
Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14/14 [01:25<00:00,  6.12s/it]
Epoch 5 Validation Loss: 0.0955

Epoch 6/10 - Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 54/54 [18:20<00:00, 20.38s/it]
Epoch 6 Training Loss: 0.0433
Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14/14 [01:23<00:00,  5.95s/it]
Epoch 6 Validation Loss: 0.0967

Epoch 7/10 - Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 54/54 [18:26<00:00, 20.49s/it]
Epoch 7 Training Loss: 0.0358
Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14/14 [01:28<00:00,  6.32s/it]
Epoch 7 Validation Loss: 0.0968

Epoch 8/10 - Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 54/54 [18:47<00:00, 20.88s/it]
Epoch 8 Training Loss: 0.0290
Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14/14 [01:27<00:00,  6.28s/it]
Epoch 8 Validation Loss: 0.0964

Epoch 9/10 - Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 54/54 [18:50<00:00, 20.94s/it]
Epoch 9 Training Loss: 0.0242
Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14/14 [01:26<00:00,  6.20s/it]
Epoch 9 Validation Loss: 0.0968

Epoch 10/10 - Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 54/54 [19:24<00:00, 21.57s/it]
Epoch 10 Training Loss: 0.0204
Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14/14 [01:27<00:00,  6.24s/it]Epoch 10 Validation Loss: 0.0981

```

```js
# 13. Save model
os.makedirs('/content/models', exist_ok=True)
torch.save(model.state_dict(), '/content/models/ingredient_classifier.pth')
print("Model training complete and saved to /content/models/ingredient_classifier.pth")
```
results:
```js
Model training complete and saved to /content/models/ingredient_classifier.pth
```

```js
class_names = [
    "background",
    "candy", "egg tart", "french fries", "chocolate", "biscuit", "popcorn", "pudding", "ice cream", "cheese butter",
    "cake", "wine", "milkshake", "coffee", "juice", "milk", "tea", "almond", "red beans", "cashew",
    "dried cranberries", "soy", "walnut", "peanut", "egg", "apple", "date", "apricot", "avocado", "banana",
    "strawberry", "cherry", "blueberry", "raspberry", "mango", "olives", "peach", "lemon", "pear", "fig",
    "pineapple", "grape", "kiwi", "melon", "orange", "watermelon", "steak", "pork", "chicken duck", "sausage",
    "fried meat", "lamb", "sauce", "crab", "fish", "shellfish", "shrimp", "soup", "bread", "corn", "hamburg",
    "pizza", "hanamaki baozi", "wonton dumplings", "pasta", "noodles", "rice", "pie", "tofu", "eggplant",
    "potato", "garlic", "cauliflower", "tomato", "kelp", "seaweed", "spring onion", "rape", "ginger", "okra",
    "lettuce", "pumpkin", "cucumber", "white radish", "carrot", "asparagus", "bamboo shoots", "broccoli",
    "celery stick", "cilantro mint", "snow peas", "cabbage", "bean sprouts", "onion", "pepper", "green beans",
    "French beans", "king oyster mushroom", "shiitake", "enoki mushroom", "oyster mushroom", "white button mushroom",
    "salad", "other ingredients"
]
```


```js
import matplotlib.pyplot as plt
import os

# å®šä¹‰æ ‡ç­¾è§£ç å™¨
def decode_labels(pred, class_names, threshold=0.5):
    return [class_names[i] for i, p in enumerate(pred) if p >= threshold]

# åˆ›å»ºè¾“å‡ºæ–‡ä»¶å¤¹
os.makedirs("/content/results", exist_ok=True)

# å¯è§†åŒ–å‰5å¼ å›¾åƒ
model.eval()
with torch.no_grad():
    for batch_idx, (imgs, labels) in enumerate(val_loader):
        imgs = imgs.to(device)
        outputs = model(imgs)
        preds = torch.sigmoid(outputs).detach().cpu().numpy()

        for i in range(min(10, imgs.size(0))):
            img = imgs[i].cpu().permute(1, 2, 0).numpy()
            img = img * [0.229, 0.224, 0.225] + [0.485, 0.456, 0.406]  # åæ ‡å‡†åŒ–
            img = np.clip(img, 0, 1)

            pred_labels = decode_labels(preds[i], class_names)
            true_labels = decode_labels(labels[i].numpy(), class_names)

            plt.figure(figsize=(5, 5))
            plt.imshow(img)
            plt.title(f"Predicted: {', '.join(pred_labels)}\nTrue: {', '.join(true_labels)}", fontsize=10)
            plt.axis('off')
            plt.tight_layout()
            plt.savefig(f"/content/results/prediction_{batch_idx}_{i}.png")
            plt.close()
        break  # åªä¿å­˜ç¬¬ä¸€ä¸ª batch çš„é¢„æµ‹
print("âœ… Results are saved in /content/results/")
```


```js
Results are saved in /content/results/
```
```js
from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score

def evaluate_model(model, dataloader, threshold=0.5):
    model.eval()
    all_preds = []
    all_labels = []

    with torch.no_grad():
        for imgs, labels in tqdm(dataloader, desc="Evaluating"):
            imgs = imgs.to(device)
            outputs = model(imgs)
            preds = torch.sigmoid(outputs).cpu().numpy()
            labels = labels.numpy()

            all_preds.append(preds)
            all_labels.append(labels)

    all_preds = np.vstack(all_preds)
    all_labels = np.vstack(all_labels)

    binary_preds = (all_preds >= threshold).astype(int)

    precision = precision_score(all_labels, binary_preds, average='micro')
    recall = recall_score(all_labels, binary_preds, average='micro')
    f1 = f1_score(all_labels, binary_preds, average='micro')

    try:
        auc = roc_auc_score(all_labels, all_preds, average='micro')
    except:
        auc = 'N/A (AUC not defined for this batch)'

    print("ğŸ” Evaluation Results:")
    print(f"Precision: {precision:.4f}")
    print(f"Recall:    {recall:.4f}")
    print(f"F1 Score:  {f1:.4f}")
    print(f"AUC:       {auc}")

# ğŸ‘‰ ä½¿ç”¨æ–¹å¼ï¼š
evaluate_model(model, val_loader)
```

```js
Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14/14 [01:31<00:00,  6.53s/it]ğŸ” Evaluation Results:
Precision: 0.8299
Recall:    0.4466
F1 Score:  0.5807
AUC:       0.9331802396996636
```



