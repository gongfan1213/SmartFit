# -*- coding: utf-8 -*-
"""24439169宫凡GongFanObject1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1XgzhGeM73PmWnIiE9tlmNuUaEsw1TgUt

# Project 2: SmartRecipe

宫凡GongFan 24439169

### Overview

This document details the SmartRecipe project, including data processing, model training, evaluation, and visualization steps. Each code section is annotated to explain its purpose and the flow of logic in building a multi-label ingredient classification model.


SmartRecipe is a revolutionary app that simplifies cooking by using advanced image recognition to identify ingredients from user photos. More than just a recipe finder, it suggests personalized dishes based on available ingredients and provides detailed nutritional insights. With SmartRecipe, meal preparation becomes effortless, food waste is reduced, and users are empowered to make smart, delicious, and healthy culinary choices. Objectives


Food Ingredient Identification: Train a multi-label image classification model that can recognize multiple ingredients present in the given image. Reference Datasets: MAFood121, Foodseg103 https://xiongweiwu.github.io/foodseg103.html



Recipe Recommendation System: Fine-tune an LLM to learn recipe recommendations. The model should analyze the input ingredients and suggest multiple feasible recipes, providing detailed information on ingredient quantities and preparation methods. Reference Dataset: Food Recipe


Nutrient Analysis System: Based on the variety of ingredients in the recipe, associate their nutritional content with the recipe information. When creating the training data for recipes recommendation, this nutritional information can be integrated to provide a more comprehensive recommendation. Reference Dataset: Product Database

### Step 1: Install Required Libraries
These libraries include transformer models, datasets, and visualization tools required for training and evaluation.
"""

# 安装必要库
!pip install -q transformers datasets torch torchvision timm
!pip install -q scikit-learn matplotlib seaborn
!pip install -q pillow

"""### Step 2: Install and Import Dependencies
This section installs core Python packages and imports all necessary modules.
"""

# 1. Install and import dependencies
!pip install --quiet kaggle torch torchvision pillow matplotlib tqdm

import os
import glob
import numpy as np
from PIL import Image
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms, models
from sklearn.model_selection import train_test_split
from tqdm import tqdm

"""1. 环境准备

Prepare sample dataset

### Step 3: Download Dataset
The FoodSeg103 dataset is downloaded and extracted for training the model.
"""

# 4. Download FoodSeg103 dataset
!wget -q https://research.larc.smu.edu.sg/downloads/datarepo/FoodSeg103.zip -P /content/datasets/
!unzip -q -P LARCdataset9947 /content/datasets/FoodSeg103.zip -d /content/datasets

"""### Step 4: Prepare Image and Mask Paths
Build lists of image files and corresponding segmentation mask files.
"""

# 5. Prepare file lists
image_dir = '/content/datasets/FoodSeg103/images'
mask_dir  = '/content/datasets/FoodSeg103/masks'
image_paths = glob.glob(os.path.join(image_dir, '*.jpg'))
mask_paths  = [os.path.join(mask_dir, os.path.basename(p).replace('.jpg', '.png')) for p in image_paths]

"""### Step 5: Automatically Detect Dataset Directories
Ensure that valid image and mask folders are detected for loading data.
"""

# 5. Automatically detect image and mask directories
root = '/content/datasets/FoodSeg103'
image_dirs = []
mask_dirs = []
for dirpath, dirnames, filenames in os.walk(root):
    if any(f.lower().endswith('.jpg') for f in filenames):
        image_dirs.append(dirpath)
    if any(f.lower().endswith('.png') for f in filenames):
        mask_dirs.append(dirpath)
if not image_dirs or not mask_dirs:
    raise RuntimeError(f"No image or mask directory found under {root}")
# Use first found directories (adjust index if needed)
image_dir = image_dirs[0]
mask_dir  = mask_dirs[0]
print(f"Using images from: {image_dir}")
print(f"Using masks from:  {mask_dir}")

"""### Step 6: Pair Images and Masks
Match image files to their corresponding annotation mask files for training.
"""

# 6. Prepare paired lists of images and masks
all_images = glob.glob(os.path.join(image_dir, '*.jpg'))
image_paths = []
mask_paths  = []
for img_path in all_images:
    mask_path = os.path.join(mask_dir, os.path.basename(img_path).replace('.jpg', '.png'))
    if os.path.exists(mask_path):
        image_paths.append(img_path)
        mask_paths.append(mask_path)
print(f"Found {len(image_paths)} paired image-mask files.")

if len(image_paths) == 0:
    raise RuntimeError("No image-mask pairs found. Please check your directory structure.")

"""### Step 7: Count Ingredient Classes
Use the masks to determine the number of unique ingredient categories.
"""

# 7. Determine number of ingredient classes
sample_mask = np.array(Image.open(mask_paths[0]))
num_classes = int(sample_mask.max()) + 1
print(f"Detected {num_classes} ingredient classes.")

"""### Step 8: Create Custom Dataset Class
Defines how image and mask data are loaded and converted to multi-label format.
"""

# 8. Dataset and DataLoader
# 8. 数据集和数据加载器

# Define a custom dataset class named FoodIngredientDataset which inherits from the PyTorch Dataset class.
# 定义一个名为 FoodIngredientDataset 的自定义数据集类，它继承自 PyTorch 的 Dataset 类。
class FoodIngredientDataset(Dataset):
    # The constructor method for the FoodIngredientDataset class.
    # FoodIngredientDataset 类的构造函数方法。
    # img_paths: A list of file paths to the input images.
    # img_paths：输入图像的文件路径列表。
    # msk_paths: A list of file paths to the corresponding masks.
    # msk_paths：对应的掩码的文件路径列表。
    # num_classes: The total number of classes in the classification task.
    # num_classes：分类任务中的总类别数。
    # transform: An optional data transformation function (default is None).
    # transform：可选的数据变换函数（默认值为 None）。
    def __init__(self, img_paths, msk_paths, num_classes, transform=None):
        self.img_paths = img_paths
        self.msk_paths = msk_paths
        self.num_classes = num_classes
        self.transform = transform

    # This method is used to get the length of the dataset, which is the number of images in this case.
    # 此方法用于获取数据集的长度，在这种情况下是图像的数量。
    def __len__(self):
        return len(self.img_paths)

    # This method is used to get an individual item from the dataset at a given index.
    # 此方法用于从数据集中获取给定索引处的单个项目。
    def __getitem__(self, idx):
        # Open the image using the PIL Image library and convert it to RGB format.
        # 使用 PIL Image 库打开图像并将其转换为 RGB 格式。
        img = Image.open(self.img_paths[idx]).convert('RGB')
        # Open the mask image, convert it to a NumPy array.
        # 打开掩码图像，将其转换为 NumPy 数组。
        mask = np.array(Image.open(self.msk_paths[idx]))
        # Get the unique values in the mask, which represent the labels present in the mask.
        # 获取掩码中的唯一值，这些值代表掩码中存在的标签。
        labels = np.unique(mask)
        # Create a multi-hot vector with a length equal to the number of classes, initialized with zeros.
        # 创建一个长度等于类别数的多热向量，初始化为零。
        multi_hot = torch.zeros(self.num_classes, dtype=torch.float32)
        # Iterate through each label in the list of unique labels.
        # 遍历唯一标签列表中的每个标签。
        for c in labels:
            # Check if the label index is within the valid range of the number of classes.
            # 检查标签索引是否在类别的有效范围内。
            if c < self.num_classes:
                # Set the corresponding position in the multi-hot vector to 1.0 to indicate the presence of that class.
                # 将多热向量中相应的位置设置为 1.0，以表示该类别的存在。
                multi_hot[c] = 1.0
        # If a data transformation function is provided, apply it to the image.
        # 如果提供了数据变换函数，则将其应用于图像。
        if self.transform:
            img = self.transform(img)
        # Return the transformed image and the multi-hot encoded label vector.
        # 返回变换后的图像和多热编码的标签向量。
        return img, multi_hot

"""### Step 9: Define Transforms and Loaders
Prepare image preprocessing steps and create training/validation loaders.
"""

# 9. Transforms and DataLoader setup
# 9. 数据变换和数据加载器设置

# Compose a series of data transformation operations.
# 组合一系列数据变换操作。
# Resize the input images to a fixed size of (224, 224) pixels.
# 将输入图像调整为固定大小 (224, 224) 像素。
# Convert the images from numpy arrays (or PIL images) to PyTorch tensors.
# 将图像从 numpy 数组（或 PIL 图像）转换为 PyTorch 张量。
# Normalize the image tensors using the given mean and standard deviation values for each channel.
# 使用给定的每个通道的均值和标准差对图像张量进行归一化。
data_transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])

# Split the image paths and mask paths into training and validation sets.
# 将图像路径和掩码路径分割为训练集和验证集。
# The test_size parameter specifies the proportion of the data to include in the validation set (20% in this case).
# test_size 参数指定包含在验证集中的数据比例（在这种情况下为 20%）。
# The random_state parameter is set for reproducibility, ensuring the same split is obtained each time the code runs.
# random_state 参数用于确保可重复性，保证每次运行代码时都能得到相同的分割结果。
train_imgs, val_imgs, train_masks, val_masks = train_test_split(
    image_paths, mask_paths, test_size=0.2, random_state=42
)

# Create a training dataset object.
# 创建一个训练数据集对象。
# The FoodIngredientDataset is assumed to be a custom dataset class that takes in image paths, mask paths,
# the number of classes, and a data transformation function.
# 假设 FoodIngredientDataset 是一个自定义的数据集类，它接受图像路径、掩码路径、
# 类别数量和一个数据变换函数。
train_dataset = FoodIngredientDataset(train_imgs, train_masks, num_classes, transform=data_transform)

# Create a validation dataset object.
# 创建一个验证数据集对象。
# Similar to the training dataset, but with the validation image and mask paths.
# 与训练数据集类似，但使用验证图像和掩码路径。
val_dataset   = FoodIngredientDataset(val_imgs,   val_masks,   num_classes, transform=data_transform)

# Create a data loader for the training dataset.
# 为训练数据集创建一个数据加载器。
# The batch_size parameter determines the number of samples in each batch (32 in this case).
# batch_size 参数确定每个批次中的样本数量（在这种情况下为 32）。
# The shuffle parameter is set to True to shuffle the data at each epoch for better training performance.
# shuffle 参数设置为 True，以便在每个 epoch 对数据进行打乱，以获得更好的训练性能。
# The num_workers parameter specifies the number of subprocesses to use for data loading (2 in this case).
# num_workers 参数指定用于数据加载的子进程数量（在这种情况下为 2）。
train_loader  = DataLoader(train_dataset, batch_size=32, shuffle=True,  num_workers=2)

# Create a data loader for the validation dataset.
# 为验证数据集创建一个数据加载器。
# Similar to the training data loader, but with shuffle set to False as there is no need to shuffle the validation data.
# 与训练数据加载器类似，但 shuffle 设置为 False，因为不需要打乱验证数据。
val_loader    = DataLoader(val_dataset,   batch_size=32, shuffle=False, num_workers=2)

"""### Step 10: Define Model Architecture
A ResNet-50 model is used and modified for multi-label classification.
"""

# 10. Model definition
model = models.resnet50(pretrained=True)
model.fc = nn.Linear(model.fc.in_features, num_classes)

"""### Step 11: Set Up GPU (if available)
Check if CUDA is available and move the model to GPU.
"""

# Enable CUDA only if available
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = model.to(device)

"""### Step 12: Define Loss Function and Optimizer
Use Binary Cross Entropy with Logits and Adam optimizer.
"""

# 11. Loss and optimizer
criterion = nn.BCEWithLogitsLoss()
optimizer = optim.Adam(model.parameters(), lr=1e-4)

"""### Step 13: Training Loop with Early Stopping
Implements model training and validation with early stopping logic.
"""

import torch
# Number of training epochs
# 训练的轮数
num_epochs = 50
# Early stopping parameter setup
# 早停参数设置
patience = 5  # Number of epochs to tolerate without improvement in validation loss
# 容忍的停滞epoch数，即验证损失没有改善的轮数
best_val_loss = float('inf')
# Initialize a counter for the number of epochs without improvement in validation loss
# 初始化一个计数器，用于记录验证损失没有改善的轮数
counter = 0
# Variable to store the best model weights
# 用于存储最佳模型参数的变量
best_model_weights = None

# Main training loop over the specified number of epochs
# 针对指定轮数的主训练循环
for epoch in range(num_epochs):
    # Set the model to training mode. In training mode, layers like dropout and batch normalization
    # behave in a way that accounts for training (e.g., dropout randomly drops neurons).
    # 将模型设置为训练模式。在训练模式下，像 dropout 和批量归一化这样的层会以适合训练的方式运行（例如，dropout 会随机丢弃神经元）。
    model.train()
    running_loss = 0.0
    # Iterate through the training data loader. tqdm is used to display a progress bar.
    # 遍历训练数据加载器。tqdm 用于显示进度条。
    for imgs, labels in tqdm(train_loader, desc=f"Epoch {epoch+1}/{num_epochs} - Training"):
        # Move the input images and corresponding labels to the specified device (e.g., GPU if available)
        # 将输入图像和对应的标签移动到指定设备（例如，如果可用则为 GPU）
        imgs, labels = imgs.to(device), labels.to(device)
        # Zero the gradients of the optimizer. This is necessary before each backward pass
        # to avoid accumulating gradients from previous batches.
        # 清零优化器的梯度。在每次反向传播之前执行此操作是必要的，以避免累积前一批次的梯度。
        optimizer.zero_grad()
        # Pass the input images through the model to get the output predictions
        # 将输入图像传入模型以获得输出预测
        outputs = model(imgs)
        # Calculate the loss between the model's output and the true labels using the defined loss function
        # 使用定义的损失函数计算模型输出和真实标签之间的损失
        loss = criterion(outputs, labels)
        # Perform backpropagation to compute the gradients of the loss with respect to the model's parameters
        # 执行反向传播，计算损失关于模型参数的梯度
        loss.backward()
        # Update the model's parameters using the optimizer
        # 使用优化器更新模型的参数
        optimizer.step()
        # Accumulate the loss for this batch. Multiply the loss value by the number of samples in the batch
        # 累积此批次的损失。将损失值乘以批次中的样本数量
        running_loss += loss.item() * imgs.size(0)
    # Calculate the average training loss for this epoch by dividing the accumulated loss by the total number of samples in the training dataset
    # 通过将累积损失除以训练数据集中的样本总数来计算此轮的平均训练损失
    epoch_loss = running_loss / len(train_loader.dataset)
    print(f"Epoch {epoch+1} Training Loss: {epoch_loss:.4f}")

    # Set the model to evaluation mode. In evaluation mode, layers like dropout and batch normalization
    # behave differently (e.g., dropout doesn't drop neurons) to ensure consistent results during validation.
    # 将模型设置为评估模式。在评估模式下，像 dropout 和批量归一化这样的层会以不同的方式运行（例如，dropout 不会丢弃神经元），以确保在验证期间结果的一致性。
    model.eval()
    val_loss = 0.0
    # Disable gradient calculation during validation to save memory and speed up the process
    # 在验证期间禁用梯度计算以节省内存并加快处理速度
    with torch.no_grad():
        # Iterate through the validation data loader. tqdm is used to display a progress bar.
        # 遍历验证数据加载器。tqdm 用于显示进度条。
        for imgs, labels in tqdm(val_loader, desc="Validation"):
            # Move the input images and corresponding labels to the specified device (e.g., GPU if available)
            # 将输入图像和对应的标签移动到指定设备（例如，如果可用则为 GPU）
            imgs, labels = imgs.to(device), labels.to(device)
            # Pass the input images through the model to get the output predictions
            # 将输入图像传入模型以获得输出预测
            outputs = model(imgs)
            # Calculate the loss between the model's output and the true labels using the defined loss function
            # 使用定义的损失函数计算模型输出和真实标签之间的损失
            loss = criterion(outputs, labels)
            # Accumulate the loss for this batch. Multiply the loss value by the number of samples in the batch
            # 累积此批次的损失。将损失值乘以批次中的样本数量
            val_loss += loss.item() * imgs.size(0)
    # Calculate the average validation loss for this epoch by dividing the accumulated loss by the total number of samples in the validation dataset
    # 通过将累积损失除以验证数据集中的样本总数来计算此轮的平均验证损失
    val_loss /= len(val_loader.dataset)
    print(f"Epoch {epoch+1} Validation Loss: {val_loss:.4f}\n")

    # Early stopping check
    # 早停检查
    if val_loss < best_val_loss:
        # Update the best validation loss if the current validation loss is lower
        # 如果当前验证损失更低，则更新最佳验证损失
        best_val_loss = val_loss
        # Save the current model's state dictionary as the best model weights
        # 将当前模型的状态字典保存为最佳模型参数
        best_model_weights = model.state_dict()
        # Reset the counter since there has been an improvement in validation loss
        # 由于验证损失有所改善，重置计数器
        counter = 0
    else:
        # Increment the counter if there is no improvement in validation loss
        # 如果验证损失没有改善，则增加计数器
        counter += 1
        print(f"Validation loss did not improve. Counter: {counter}/{patience}")
        if counter >= patience:
            print(f"Early stopping triggered at epoch {epoch+1}!")
            break

# Load the best model weights after training is completed
# 在训练完成后加载最佳模型参数
model.load_state_dict(best_model_weights)
print("Training completed. Best validation loss: {:.4f}".format(best_val_loss))

"""### Step 14: Save the Trained Model
Model weights are saved after training for future inference.
"""

# 13. Save model
os.makedirs('/content/models', exist_ok=True)
torch.save(model.state_dict(), '/content/models/ingredient_classifier1.pth')
print("Model training complete and saved to /content/models/ingredient_classifier1.pth")

class_names = [
    "background",
    "candy", "egg tart", "french fries", "chocolate", "biscuit", "popcorn", "pudding", "ice cream", "cheese butter",
    "cake", "wine", "milkshake", "coffee", "juice", "milk", "tea", "almond", "red beans", "cashew",
    "dried cranberries", "soy", "walnut", "peanut", "egg", "apple", "date", "apricot", "avocado", "banana",
    "strawberry", "cherry", "blueberry", "raspberry", "mango", "olives", "peach", "lemon", "pear", "fig",
    "pineapple", "grape", "kiwi", "melon", "orange", "watermelon", "steak", "pork", "chicken duck", "sausage",
    "fried meat", "lamb", "sauce", "crab", "fish", "shellfish", "shrimp", "soup", "bread", "corn", "hamburg",
    "pizza", "hanamaki baozi", "wonton dumplings", "pasta", "noodles", "rice", "pie", "tofu", "eggplant",
    "potato", "garlic", "cauliflower", "tomato", "kelp", "seaweed", "spring onion", "rape", "ginger", "okra",
    "lettuce", "pumpkin", "cucumber", "white radish", "carrot", "asparagus", "bamboo shoots", "broccoli",
    "celery stick", "cilantro mint", "snow peas", "cabbage", "bean sprouts", "onion", "pepper", "green beans",
    "French beans", "king oyster mushroom", "shiitake", "enoki mushroom", "oyster mushroom", "white button mushroom",
    "salad", "other ingredients"
]

"""1. 预测结果可视化并保存到 /content/results/

### Step 15: Prediction Visualization
Visualizes predictions by decoding model outputs and comparing them with ground truth labels.
"""

import matplotlib.pyplot as plt
import os

# Define a function to decode the predicted labels.
# This function takes the predicted probabilities (pred), a list of class names (class_names),
# and a threshold value (default is 0.5). It returns a list of class names for which the predicted
# probabilities are greater than or equal to the threshold.
# 定义一个函数来解码预测的标签。
# 该函数接受预测概率（pred）、类别名称列表（class_names）以及阈值（默认值为 0.5）。
# 它返回预测概率大于或等于阈值的类别名称列表。
def decode_labels(pred, class_names, threshold=0.5):
    return [class_names[i] for i, p in enumerate(pred) if p >= threshold]

# Create the output folder. If the folder already exists, the 'exist_ok=True' parameter ensures
# that no error will be raised.
# 创建输出文件夹。如果文件夹已经存在，'exist_ok=True' 参数可确保不会引发错误。
os.makedirs("/content/results", exist_ok=True)

# Visualize the first 5 images. Set the model to evaluation mode.
# 在评估模式下可视化前 5 张图像，将模型设置为评估模式。
model.eval()

# Disable gradient calculation during inference to save memory and speed up the process.
# 在推理过程中禁用梯度计算以节省内存并加快处理速度。
with torch.no_grad():
    # Iterate through the validation data loader.
    # 遍历验证数据加载器。
    for batch_idx, (imgs, labels) in enumerate(val_loader):
        # Move the input images to the specified device (e.g., GPU if available).
        # 将输入图像移动到指定设备（例如，如果可用则为 GPU）。
        imgs = imgs.to(device)
        # Pass the images through the model to get the output logits.
        # 将图像传入模型以获得输出的 logits。
        outputs = model(imgs)
        # Apply the sigmoid function to the output logits to convert them into probabilities,
        # detach from the computation graph, and convert to a NumPy array.
        # 对输出的 logits 应用 sigmoid 函数，将其转换为概率，从计算图中分离并转换为 NumPy 数组。
        preds = torch.sigmoid(outputs).detach().cpu().numpy()

        # Iterate through a subset of the images in the current batch.
        # The number of images to iterate over is the minimum of 35 and the number of images in the batch.
        # 遍历当前批次中的一部分图像。
        # 要遍历的图像数量是 35 和批次中图像数量的最小值。
        for i in range(min(35, imgs.size(0))):
            # Move the i-th image back to the CPU, rearrange its dimensions from (C, H, W) to (H, W, C),
            # and convert it to a NumPy array.
            # 将第 i 张图像移回 CPU，将其维度从 (C, H, W) 重新排列为 (H, W, C)，然后转换为 NumPy 数组。
            img = imgs[i].cpu().permute(1, 2, 0).numpy()
            # Reverse the normalization process. Multiply by the standard deviation and add the mean.
            # 对归一化过程进行逆操作。乘以标准差并加上均值。
            img = img * [0.229, 0.224, 0.225] + [0.485, 0.456, 0.406]
            # Clip the pixel values to ensure they are within the range [0, 1].
            # 裁剪像素值以确保它们在 [0, 1] 范围内。
            img = np.clip(img, 0, 1)

            # Decode the predicted labels using the defined function.
            # 使用定义的函数解码预测的标签。
            pred_labels = decode_labels(preds[i], class_names)
            # Decode the true labels using the defined function.
            # 使用定义的函数解码真实标签。
            true_labels = decode_labels(labels[i].numpy(), class_names)

            # Create a new figure with a specified size.
            # 创建一个指定大小的新图形。
            plt.figure(figsize=(5, 5))
            # Display the image.
            # 显示图像。
            plt.imshow(img)
            # Set the title of the figure with the predicted and true labels.
            # 使用预测标签和真实标签设置图形的标题。
            plt.title(f"Predicted: {', '.join(pred_labels)}\nTrue: {', '.join(true_labels)}", fontsize=10)
            # Turn off the axis labels and ticks.
            # 关闭坐标轴标签和刻度。
            plt.axis('off')
            # Adjust the subplot parameters to give specified padding.
            # 调整子图参数以提供指定的填充。
            plt.tight_layout()
            # Save the figure as an image file with a specific naming convention.
            # 以特定的命名约定将图形保存为图像文件。
            plt.savefig(f"/content/results/prediction_{batch_idx}_{i}.png")
            # Close the figure to free up memory.
            # 关闭图形以释放内存。
            plt.close()
        # Break after processing the first batch to only save predictions for the first batch.
        # 处理完第一个批次后跳出循环，以便只保存第一个批次的预测结果。
        break
print("✅ 示例预测结果已保存到 /content/results/")

"""2. 计算 Precision / Recall / F1-score

### Step 16: Calculate Precision, Recall, and F1-Score

Evaluate classification performance using common multi-label metrics.
"""

# Import precision_score, recall_score, and f1_score functions from sklearn.metrics library.
# These functions are used to calculate precision, recall, and F1-score metrics respectively.
# 从 sklearn.metrics 库中导入 precision_score、recall_score 和 f1_score 函数。
# 这些函数分别用于计算精确率、召回率和 F1 分数指标。
from sklearn.metrics import precision_score, recall_score, f1_score

# Initialize empty lists to store true labels and predicted labels.
# 初始化空列表，用于存储真实标签和预测标签。
y_true, y_pred = [], []

# Set the model to evaluation mode. This is important as it disables certain layers like dropout
# or batch normalization layers from working in training mode, ensuring consistent inference results.
# 将模型设置为评估模式。这很重要，因为它会禁用某些层（如 dropout 或批量归一化层）在训练模式下的工作，
# 确保推理结果的一致性。
model.eval()

# Use torch.no_grad() context manager to disable gradient calculation during inference.
# This reduces memory usage and speeds up the inference process.
# 使用 torch.no_grad() 上下文管理器在推理期间禁用梯度计算。
# 这可以减少内存使用并加快推理过程。
with torch.no_grad():
    # Iterate through the validation data loader to get batches of images and corresponding labels.
    # 遍历验证数据加载器，获取一批图像和对应的标签。
    for imgs, labels in val_loader:
        # Move the input images to the specified device (e.g., GPU if available).
        # 将输入图像移动到指定设备（例如，如果可用则为 GPU）。
        imgs = imgs.to(device)
        # Pass the images through the model to get the output logits.
        # 将图像传入模型以获得输出的 logits。
        outputs = model(imgs)
        # Apply the sigmoid function to the output logits to convert them into probabilities in the range [0, 1].
        # Then detach the tensor from the computation graph and convert it to a NumPy array.
        # 对输出的 logits 应用 sigmoid 函数，将其转换为范围在 [0, 1] 内的概率。
        # 然后将张量从计算图中分离并转换为 NumPy 数组。
        preds = torch.sigmoid(outputs).cpu().detach().numpy()
        # Append the predicted probabilities to the y_pred list.
        # 将预测概率添加到 y_pred 列表中。
        y_pred.append(preds)
        # Convert the true labels to NumPy array and append them to the y_true list.
        # 将真实标签转换为 NumPy 数组并添加到 y_true 列表中。
        y_true.append(labels.numpy())

# Vertically stack the lists of predicted probabilities and true labels into 2D NumPy arrays.
# This combines all the batches of predictions and true labels into single arrays for further processing.
# 将预测概率列表和真实标签列表垂直堆叠成二维 NumPy 数组。
# 这将所有批次的预测和真实标签合并为单个数组，以便进行进一步处理。
y_pred = np.vstack(y_pred)
y_true = np.vstack(y_true)

# Binarize the predicted probabilities by setting a threshold of 0.5.
# If the probability is greater than or equal to 0.5, it is considered a positive prediction (1); otherwise, a negative prediction (0).
# 通过设置阈值 0.5 对预测概率进行二值化。
# 如果概率大于或等于 0.5，则认为是正预测（1）；否则，为负预测（0）。
y_pred_bin = (y_pred >= 0.5).astype(int)

# Calculate the precision, recall, and F1-score for each class separately.
# The 'average=None' parameter indicates that the metrics should be calculated for each individual class.
# 分别为每个类别计算精确率、召回率和 F1 分数。
# 'average=None' 参数表示应针对每个单独的类别计算指标。
precision = precision_score(y_true, y_pred_bin, average=None)
recall = recall_score(y_true, y_pred_bin, average=None)
f1 = f1_score(y_true, y_pred_bin, average=None)

# Iterate through the list of class names.
# For each class, retrieve the corresponding precision, recall, and F1-score values.
# If the index is out of bounds for the metric arrays, set the value to 0.
# Then print out the class name along with its precision, recall, and F1-score values.
# 遍历类别名称列表。
# 对于每个类别，检索相应的精确率、召回率和 F1 分数值。
# 如果索引超出指标数组的范围，则将值设置为 0。
# 然后打印出类别名称及其精确率、召回率和 F1 分数值。
for i in range(len(class_names)):
    p = precision[i] if i < len(precision) else 0
    r = recall[i] if i < len(recall) else 0
    f = f1[i] if i < len(f1) else 0
    print(f"{class_names[i]}: Precision={p:.2f}, Recall={r:.2f}, F1={f:.2f}")

"""### Step 17: Multilabel Confusion Matrix and ROC AUC

Compute confusion matrix and AUC scores to understand model prediction confidence and accuracy.
"""

# Import the necessary functions from the sklearn.metrics library.
# multilabel_confusion_matrix is used to compute a confusion matrix for each label in a multi-label classification problem.
# roc_auc_score is used to calculate the area under the receiver operating characteristic curve (ROC AUC) for multi-label classification.
# 从 sklearn.metrics 库中导入必要的函数。
# multilabel_confusion_matrix 用于为多标签分类问题中的每个标签计算混淆矩阵。
# roc_auc_score 用于计算多标签分类的受试者工作特征曲线下面积（ROC AUC）。
from sklearn.metrics import multilabel_confusion_matrix, roc_auc_score

# Calculate the multi-label confusion matrix.
# This function takes the true labels (y_true) and the predicted binary labels (y_pred_bin)
# and returns a confusion matrix for each individual label.
# 计算多标签混淆矩阵。
# 此函数接受真实标签（y_true）和预测的二进制标签（y_pred_bin），
# 并为每个单独的标签返回一个混淆矩阵。
mcm = multilabel_confusion_matrix(y_true, y_pred_bin)

# Iterate through each confusion matrix in the multi-label confusion matrix.
# For each label, extract the true negatives (tn), false positives (fp), false negatives (fn), and true positives (tp).
# Then print out these values along with the corresponding class name.
# 遍历多标签混淆矩阵中的每个混淆矩阵。
# 对于每个标签，提取真阴性（tn）、假阳性（fp）、假阴性（fn）和真阳性（tp）。
# 然后打印出这些值以及相应的类别名称。
for i, cm in enumerate(mcm):
    tn, fp, fn, tp = cm.ravel()
    print(f"{class_names[i]}: TP={tp}, FP={fp}, FN={fn}, TN={tn}")

# Calculate the multi-label Area Under the Curve (AUC) using the macro-averaging method.
# The macro-averaging method calculates the AUC for each label separately and then takes the unweighted mean.
# Wrap the calculation in a try-except block to handle potential errors, such as when some classes have all zero labels.
# 使用宏平均方法计算多标签的曲线下面积（AUC）。
# 宏平均方法分别计算每个标签的 AUC，然后取未加权的平均值。
# 将计算过程包装在 try-except 块中，以处理潜在的错误，例如某些类别的标签全为零的情况。
try:
    auc = roc_auc_score(y_true, y_pred, average="macro")
    print(f"Macro AUC: {auc:.4f}")
except ValueError as e:
    print("⚠️ AUC calculation failed: Some classes may have all zero labels.")

"""### Step 18: Example Prediction Visualization
Demonstrate and display a batch of prediction results in visual format.
"""

# Set the model to evaluation mode. In this mode, certain layers like dropout or batch normalization
# behave differently compared to training mode. For example, dropout won't randomly zero out neurons.
# 将模型设置为评估模式。在此模式下，像 dropout 或批量归一化等某些层的行为与训练模式不同。
# 例如，dropout 不会随机将神经元置零。
model.eval()

# Retrieve a single batch of data from the validation data loader.
# The validation data loader is an iterator that provides batches of data during the validation phase.
# 从验证数据加载器中获取一批数据。验证数据加载器是一个迭代器，在验证阶段提供一批批数据。
imgs, labels = next(iter(val_loader))

# Move the input images and corresponding labels to the specified device (e.g., GPU if available).
# This ensures that the model and the data are on the same device for computation.
# 将输入图像和对应的标签移动到指定设备（例如，如果可用则为 GPU）。
# 这确保了模型和数据在同一设备上进行计算。
imgs, labels = imgs.to(device), labels.to(device)

# Begin a context where PyTorch doesn't compute gradients.
# This is useful for inference because gradient computation is not needed during prediction,
# which saves memory and speeds up the process.
# 开启一个上下文，在该上下文中 PyTorch 不计算梯度。
# 这在推理阶段很有用，因为预测时不需要进行梯度计算，这样可以节省内存并加快处理速度。
with torch.no_grad():
    # Pass the input images through the model to get the raw output logits.
    # These logits represent the model's unnormalized predictions for each class.
    # 将输入图像传入模型以获得原始输出的 logits。
    # 这些 logits 表示模型对每个类别的未归一化预测。
    outputs = model(imgs)
    # Apply the sigmoid function to the outputs to convert the logits into probabilities
    # in the range [0, 1]. Then detach the tensor from the computation graph and convert it
    # to a NumPy array for further processing outside of PyTorch.
    # 对输出应用 sigmoid 函数，将 logits 转换为范围在 [0, 1] 内的概率。
    # 然后将张量从计算图中分离，并将其转换为 NumPy 数组，以便在 PyTorch 之外进行进一步处理。
    preds = torch.sigmoid(outputs).detach().cpu().numpy()

# Dynamically obtain the size of the current batch of data.
# This can vary depending on the configuration of the data loader.
# 动态获取当前这批数据的大小。这可能会根据数据加载器的配置而有所不同。
batch_size = imgs.size(0)

# Set the maximum number of images to display. This is used to limit the number of visualizations
# if the batch size is larger than desired.
# 设置要显示的最大图像数量。如果批次大小大于期望数量，这用于限制可视化的数量。
max_show = 3

# Iterate through the images in the batch, but limit the iteration to the minimum of the batch size
# and the maximum number of images to display.
# 遍历批次中的图像，但将迭代次数限制为批次大小和最大显示图像数量中的较小值。
for i in range(min(max_show, batch_size)):
    # Move the i-th image back to the CPU, rearrange its dimensions from (C, H, W) to (H, W, C)
    # which is the format expected by Matplotlib, and convert it to a NumPy array.
    # 将第 i 张图像移回 CPU，将其维度从 (C, H, W) 重新排列为 (H, W, C)，
    # 这是 Matplotlib 期望的格式，然后将其转换为 NumPy 数组。
    img = imgs[i].cpu().permute(1, 2, 0).numpy()

    # Reverse the normalization process that was likely applied during data preprocessing.
    # Multiply by the standard deviation and add the mean to get the original pixel values.
    # 对在数据预处理期间可能应用的归一化过程进行逆操作。
    # 乘以标准差并加上均值以得到原始像素值。
    img = img * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])

    # Clip the pixel values to ensure they are within the valid range of [0, 1].
    # This is necessary because numerical errors during computation might cause values to go out of bounds.
    # 裁剪像素值以确保它们在有效范围 [0, 1] 内。
    # 这是必要的，因为计算过程中的数值误差可能会导致值超出范围。
    img = np.clip(img, 0, 1)

    # Use a custom function `decode_labels` to convert the predicted probabilities (from the model output)
    # into human-readable class names. The function should take the probabilities and a list of all class names
    # as inputs.
    # 使用自定义函数 `decode_labels` 将预测概率（来自模型输出）转换为人类可读的类别名称。
    # 该函数应将概率和所有类别名称的列表作为输入。
    pred_labels = decode_labels(preds[i], class_names)

    # Similarly, use the `decode_labels` function to convert the true labels (from the data loader)
    # into human-readable class names.
    # 同样，使用 `decode_labels` 函数将真实标签（来自数据加载器）转换为人类可读的类别名称。
    true_labels = decode_labels(labels[i].cpu().numpy(), class_names)

    # Create a new Matplotlib figure with a specified size.
    # This will be used to display the image and its associated prediction and true label information.
    # 创建一个具有指定大小的新 Matplotlib 图形。
    # 这将用于显示图像及其相关的预测和真实标签信息。
    plt.figure(figsize=(5, 5))

    # Display the image using Matplotlib's `imshow` function.
    # 使用 Matplotlib 的 `imshow` 函数显示图像。
    plt.imshow(img)

    # Set the title of the figure to show the predicted and true class names.
    # The `join` method is used to concatenate the list of class names into a single string.
    # 设置图形的标题以显示预测和真实的类别名称。
    # `join` 方法用于将类别名称列表连接成一个字符串。
    plt.title(f"Predicted: {', '.join(pred_labels)}\nTrue: {', '.join(true_labels)}")

    # Turn off the axis labels and ticks to make the visualization cleaner.
    # 关闭坐标轴标签和刻度，使可视化效果更简洁。
    plt.axis("off")

    # Display the figure.
    # 显示图形。
    plt.show()

"""# 1. 添加预测测试与可视化结果
展示模型预测的实际效果（图片 + 预测标签 vs. 真实标签）

### Step 18: Example Prediction Visualization
Demonstrate and display a batch of prediction results in visual format.
"""

import torch
import numpy as np
import matplotlib.pyplot as plt

# Ensure the model is in evaluation mode
# 确保模型处于评估模式
model.eval()

# Get a batch of validation data (automatically adapts to the current batch_size)
# 获取一批验证数据（自动适配当前batch_size）
imgs, labels = next(iter(val_loader))
# Move the images and labels to the specified device (e.g., GPU)
# 将图像和标签移动到指定设备（如 GPU）
imgs, labels = imgs.to(device), labels.to(device)

# Make predictions
# 预测结果
with torch.no_grad():
    # Pass the images through the model to get the raw outputs
    # 将图像输入模型，得到原始输出
    outputs = model(imgs)
    # Apply the sigmoid function to the outputs, detach from the computation graph,
    # and convert to a numpy array for multi-label classification predictions
    # 对输出应用 sigmoid 函数，从计算图中分离，然后转换为 numpy 数组，用于多标签分类预测
    preds = torch.sigmoid(outputs).detach().cpu().numpy()

# Dynamically get the batch size
# 动态获取批次大小
batch_size = imgs.size(0)  # The actual current batch size
max_show = 30  # Maximum number of images to display, here set to 3 for example
# 最多显示3张图片，这里例如设置为3

# Display the prediction results
# 显示预测结果
for i in range(min(max_show, batch_size)):
    # --- Image processing part ---
    # --- 图像处理部分 ---
    # Convert the tensor to a numpy array and adjust the dimension order (C, H, W) -> (H, W, C)
    # 将张量转为numpy并调整维度顺序 (C, H, W) -> (H, W, C)
    img = imgs[i].cpu().permute(1, 2, 0).numpy()

    # Reverse normalization (assuming using ImageNet mean and standard deviation)
    # 反归一化（假设使用ImageNet均值和标准差）
    img = img * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])
    # Clip the pixel values to ensure they are in the range [0, 1]
    # 确保像素值在[0,1]范围内
    img = np.clip(img, 0, 1)

    # --- Label processing part ---
    # --- 标签处理部分 ---
    # Assume the decode_labels function converts probabilities to class names
    # preds[i] should be a numpy array with shape (num_classes,)
    # 假设 decode_labels 函数将概率转换为类别名称
    # preds[i] 应该是形状为 (num_classes,) 的numpy数组
    pred_labels = decode_labels(preds[i], class_names)  # Custom decoding function
    # 自定义解码函数

    # The true labels need to be converted from the tensor
    # 真实标签需要从tensor转换
    true_labels = decode_labels(labels[i].cpu().numpy(), class_names)

    # --- Visualization part ---
    # --- 可视化部分 ---
    plt.figure(figsize=(5, 5))
    plt.imshow(img)
    plt.title(f"Predicted: {', '.join(pred_labels)}\nTrue: {', '.join(true_labels)}")
    plt.axis("off")
    plt.show()

"""# 2. 评估指标报告（精确率、召回率、F1 分数）
使用 sklearn.metrics 对模型性能进行全面评估：
"""

# Import necessary functions from sklearn.metrics for calculating evaluation metrics
# 从 sklearn.metrics 导入计算评估指标所需的函数
from sklearn.metrics import precision_score, recall_score, f1_score

# Initialize empty lists to store true labels and predicted labels
# 初始化空列表，用于存储真实标签和预测标签
y_true, y_pred = [], []

# Use torch.no_grad() to disable gradient computation during inference
# 使用 torch.no_grad() 在推理过程中禁用梯度计算，以节省内存和提高计算速度
with torch.no_grad():
    # Iterate through the validation data loader
    # 遍历验证数据集的数据加载器
    for imgs, labels in val_loader:
        # Move the input images to the specified device (e.g., GPU)
        # 将输入图像移动到指定设备（如 GPU）
        imgs = imgs.to(device)
        # Pass the images through the model to get the raw outputs
        # 将图像输入模型，得到原始输出
        outputs = model(imgs)
        # Apply the sigmoid function to the outputs and convert them to numpy arrays
        # 对输出应用 sigmoid 函数，并将其转换为 numpy 数组
        preds = torch.sigmoid(outputs).cpu().numpy()
        # Convert the predicted probabilities to binary predictions (0 or 1)
        # by thresholding at 0.5, and extend the y_pred list
        # 通过阈值 0.5 将预测概率转换为二进制预测（0 或 1），并添加到 y_pred 列表中
        y_pred.extend((preds > 0.5).astype(int))
        # Convert the true labels to numpy arrays and extend the y_true list
        # 将真实标签转换为 numpy 数组，并添加到 y_true 列表中
        y_true.extend(labels.numpy())

# Calculate and print the precision score using the 'micro' averaging method
# 计算并打印使用 'micro' 平均方法的精确率
print("Precision:", precision_score(y_true, y_pred, average='micro'))
# Calculate and print the recall score using the 'micro' averaging method
# 计算并打印使用 'micro' 平均方法的召回率
print("Recall:", recall_score(y_true, y_pred, average='micro'))
# Calculate and print the F1 score using the 'micro' averaging method
# 计算并打印使用 'micro' 平均方法的 F1 分数
print("F1 Score:", f1_score(y_true, y_pred, average='micro'))

import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve, auc
from tqdm import tqdm

# Ensure the model is in evaluation mode
# 确保模型处于评估模式
model.eval()

# Initialize containers to store all predictions and true labels
# 初始化存储所有预测和真实标签的容器
all_preds = []
all_labels = []

# Iterate through the validation set to get all predictions
# 遍历验证集获取所有预测结果
with torch.no_grad():
    for imgs, labels in tqdm(val_loader, desc="Collecting predictions"):
        # Move images and labels to the specified device (e.g., GPU)
        # 将图像和标签移动到指定设备（如 GPU）
        imgs, labels = imgs.to(device), labels.to(device)
        # Get model outputs
        # 获取模型输出
        outputs = model(imgs)
        # Apply sigmoid function to outputs and convert to numpy array
        # 对输出应用 sigmoid 函数并转换为 numpy 数组
        preds = torch.sigmoid(outputs).detach().cpu().numpy()
        # Append predictions to the container
        # 将预测结果添加到容器中
        all_preds.append(preds)
        # Append true labels to the container
        # 将真实标签添加到容器中
        all_labels.append(labels.cpu().numpy())

# Concatenate all batches of predictions and labels
# 合并所有批次的预测和标签
all_preds = np.concatenate(all_preds, axis=0)  # Shape (num_samples, num_classes)
all_labels = np.concatenate(all_labels, axis=0)  # Shape (num_samples, num_classes)

# Calculate the ROC curve and AUC for each class
# 计算每个类别的ROC曲线和AUC
num_classes = all_labels.shape[1]
# Create a figure for plotting
# 创建一个用于绘图的图形
plt.figure(figsize=(10, 8))

for class_idx in range(num_classes):
    # Get the true labels and predicted probabilities for the current class
    # 获取当前类别的真实标签和预测概率
    y_true = all_labels[:, class_idx]
    y_score = all_preds[:, class_idx]

    # Calculate the ROC curve
    # 计算ROC曲线
    fpr, tpr, _ = roc_curve(y_true, y_score)
    # Calculate the AUC of the ROC curve
    # 计算ROC曲线的AUC
    roc_auc = auc(fpr, tpr)

    # Plot the ROC curve
    # 绘制ROC曲线
    plt.plot(fpr, tpr, lw=2,
             label=f'Class {class_idx} ({class_names[class_idx]}) AUC = {roc_auc:.3f}')

# Plot the diagonal reference line
# 绘制对角线参考线
plt.plot([0, 1], [0, 1], color='gray', lw=1, linestyle='--')
# Set the x-axis limits
# 设置 x 轴范围
plt.xlim([0.0, 1.0])
# Set the y-axis limits
# 设置 y 轴范围
plt.ylim([0.0, 1.05])
# Set the x-axis label
# 设置 x 轴标签
plt.xlabel('False Positive Rate')
# Set the y-axis label
# 设置 y 轴标签
plt.ylabel('True Positive Rate')
# Set the title of the plot
# 设置图形标题
plt.title('ROC Curves per Class')
# Add a legend to the plot
# 在图形中添加图例
plt.legend(loc="lower right", fontsize=8)
# Display the plot
# 显示图形
plt.show()



# 主程序
if __name__ == "__main__":
    # Step 4: Prepare dataset paths
    root = '/content/datasets/FoodSeg103'
    image_dir = os.path.join(root, 'Images/img_dir/train')

    # 获取图像文件列表
    image_files = [os.path.join(image_dir, f) for f in os.listdir(image_dir) if f.endswith(('.png', '.jpg', '.jpeg'))]

    # 确保至少处理30张图像
    if len(image_files) < 30:
        print("图像数量不足30张，请检查数据集。")
    else:
        image_files = image_files[:30]

    # 处理每张图像
    for image_path in image_files:
        predicted_ingredients = image_recognition(image_path)
        img = Image.open(image_path)
        plt.imshow(img)
        plt.axis('off')
        plt.title(f"Predicted: {', '.join(predicted_ingredients)}")
        plt.show()